{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0564389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Expected data path and required files: \n",
    "\n",
    "DATA_DIR\n",
    "├── UK_65\n",
    "│    ├── \"dataset.csv\"\n",
    "│    ├── \"to_censure.pkl\"\n",
    "│    ├── \"inactive_ids.pkl\"\n",
    "│    ├── \"stats.pkl\"\n",
    "│    ├── \"deeper_main_meds.csv\"\n",
    "│    └── \"all_meds_with_counts.csv\"\n",
    "│    \n",
    "├── UK_70\n",
    "├── FR_65\n",
    "└── FR_70\n",
    "\"\"\"\n",
    "\n",
    "DATA_DIR =  '../../../data/datasets'\n",
    "OUTPUT_DIR = '../../../data/results/'\n",
    "SEED = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fractions import Fraction\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, precision_recall_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot like R\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Helvetica']\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "sns.set_palette('pastel')\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASES_OF_INTEREST = [\n",
    "    'alzheimer',\n",
    "    #'parkinson',\n",
    "    # 'vascular_dementias',\n",
    "    #'mci',\n",
    "    # 'alcohol_dementias',\n",
    "    # 'frontotemporal_dementias',\n",
    "    # 'other_dementias',\n",
    "    # 'parkinson_dementias',\n",
    "    'all_dementias',\n",
    "    'all_dementias+mci'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8de6cf",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365d42a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTORS = [\n",
    "'A06','A10','B03','G04','N02','N03','N05','N06','mci_at_baseline'\n",
    "]\n",
    "with open(os.path.join(OUTPUT_DIR, 'datasets_prediction.pkl'), 'rb') as f:\n",
    "    DATASETS_PRED = pickle.load(f)\n",
    "\n",
    "def get_data(country:str, age:int, disease:str, include_charlson_bmi:bool, \n",
    "             pred_up_to_year:int, include_deceased_as_zeros:bool=False, base_dir:str=DATA_DIR):\n",
    "    \"\"\"\n",
    "    Load and prepare data for prediction task.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    country : str\n",
    "        'UK' or 'FR'\n",
    "    age : int\n",
    "        65 or 70\n",
    "    disease : str\n",
    "        Disease of interest from DISEASES_OF_INTEREST or 'any'\n",
    "    include_charlson_bmi : bool\n",
    "        Whether to include BMI and CHARLSON features\n",
    "    pred_up_to_year : int\n",
    "        Prediction horizon in years\n",
    "    include_deceased_as_zeros : bool, default=False\n",
    "        If True, include patients who died during prediction period without disease as negatives (0)\n",
    "        If False, exclude patients who died during prediction period without disease (original behavior)\n",
    "    base_dir : str\n",
    "        Base directory for data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : DataFrame\n",
    "        Features\n",
    "    y : Series\n",
    "        Target labels (1=disease, 0=no disease)\n",
    "    \"\"\"\n",
    "    assert country in {'UK', 'FR'}, f'`country` must be either UK or FR, not {country}.'\n",
    "    assert age in {65, 70}, f'`age` must be either 65 or 70, not {age}.'\n",
    "    assert disease in DISEASES_OF_INTEREST or disease=='any', f'`disease` must be either in DISEASES_OF_INTEREST' \n",
    "    f'({DISEASES_OF_INTEREST}) or any (at least one neurodegenerative diseases), not {age}.'\n",
    "    \n",
    "    ## Load corresponding dataset\n",
    "    dataset = DATASETS_PRED[age][country].copy()\n",
    "    \n",
    "    # filter out patients without bmi or charlson record\n",
    "    if include_charlson_bmi:\n",
    "        mask = dataset[['avg. BMI', 'avg. CHARLSON']].notna().all(axis=1)\n",
    "        n = mask.sum()\n",
    "        print(f'* Filtering out patients without BMI or CHARLSON record ({len(dataset)-n:,} patients {(1-n/len(dataset))*100:.2f} %)')\n",
    "        print(f'\\t{len(dataset):,} -> {n:,} patients')\n",
    "        dataset = dataset.loc[mask]\n",
    "    \n",
    "    ## Filter out patients who died or became inactive / temporaire before a potential disease\n",
    "    pred_up_to_days = pred_up_to_year*365.25\n",
    "    \n",
    "    # sick\n",
    "    has_disease = ( lambda x: len(x) > 0 ) if disease=='any' else ( lambda x: any(d == disease for d, _ in x) )   \n",
    "    time_to_first_disease = lambda x: min(x, key=lambda x: x[1])[1]\n",
    "    f = lambda x: has_disease(x) and (time_to_first_disease(x) <= pred_up_to_days)\n",
    "    \n",
    "    mask_sick = dataset['diseases'].apply(f)\n",
    "    \n",
    "    # dead or inactive\n",
    "    mask_die_inac = (~dataset['person_state_code'].eq('A')) & (dataset['duration (days)'] <= pred_up_to_days)\n",
    "    \n",
    "    if include_deceased_as_zeros:\n",
    "        # NEW APPROACH: Keep ALL patients, including those who died without disease (counted as 0)\n",
    "        n_deaths_total = mask_die_inac.sum()\n",
    "        n_deaths_with_disease = (mask_die_inac & mask_sick).sum()\n",
    "        n_deaths_without_disease = (mask_die_inac & (~mask_sick)).sum()\n",
    "        \n",
    "        print(f'* Including ALL patients, including {n_deaths_total:,} deceased during period ({n_deaths_total/len(dataset)*100:.2f} %)')\n",
    "        print(f'  - {n_deaths_with_disease:,} deceased WITH disease → counted as \"1\"')\n",
    "        print(f'  - {n_deaths_without_disease:,} deceased WITHOUT disease → counted as \"0\"')\n",
    "        print(f'\\tNo patients excluded: {len(dataset):,} patients kept')\n",
    "        \n",
    "        # No filtering - keep all patients\n",
    "        filter_out = pd.Series(False, index=dataset.index)\n",
    "    else:\n",
    "        # ORIGINAL APPROACH: Exclude patients who died without developing the disease\n",
    "        filter_out = mask_die_inac & (~mask_sick)\n",
    "        n = filter_out.sum()\n",
    "        print(f'* Filtering out patients who died or became inactive before the disease of interest ({n:,} patients {n/len(dataset)*100:.2f} %)')\n",
    "        print(f'\\t{len(dataset):,} -> {len(dataset)-n:,} patients')\n",
    "    \n",
    "    dataset = dataset.loc[~filter_out]\n",
    "    \n",
    "    dataset['is.female'] = dataset.pop('gender_code').eq('F').astype(int)\n",
    "    \n",
    "    ## X, y\n",
    "    y = mask_sick.loc[~filter_out].astype(int)\n",
    "    \n",
    "    predictors = ['is.female'] + PREDICTORS\n",
    "    if include_charlson_bmi:\n",
    "        predictors = ['avg. BMI', 'avg. CHARLSON'] + predictors\n",
    "    \n",
    "    X = dataset[predictors] #TODO: add time to first_disease and person_id to analyse error ?\n",
    "    \n",
    "    n = y.sum()\n",
    "    print(f\"\\n{len(X):,} patients among which {n:,} with the disease of interest ({n/len(X)*100:.2f} %)\")\n",
    "    \n",
    "    if include_deceased_as_zeros and mask_die_inac.sum() > 0:\n",
    "        n_deaths_without_disease = (mask_die_inac.loc[~filter_out] & (~y.astype(bool))).sum()\n",
    "        print(f\"Note: Includes {n_deaths_without_disease:,} patients who died without the disease (counted as negatives)\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def split(X, y, seed=SEED):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "    \n",
    "    n_train, n_test = len(y_train), len(y_test)\n",
    "    class1_train, class1_test = y_train.sum(), y_test.sum()\n",
    "    print(f\"\"\"Datasets proportions:\n",
    "    - {class1_train:,} diseases in the train set out of {n_train:,} patients ({class1_train/n_train*100:.2f} %)\n",
    "    - {class1_test:,} diseases in the test set out of {n_test:,} patients ({class1_test/n_test*100:.2f} %)\"\"\")\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b621043",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24312e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, roc_auc, ax):\n",
    "    ax.plot(fpr, tpr, 'o-', color='navy', lw=2, label=f'AUC = {roc_auc}')\n",
    "    ax.plot([0, 1], [0, 1], color='black', lw=1)\n",
    "    ax.set_xlim(0, 1.01)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('Detection rate')\n",
    "    # ax.set_title('Receiver Operating Characteristic')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "def plot_density(preds, test, ax, label='Disease'):\n",
    "    sns.kdeplot(preds[test == 1], fill=True, ax=ax, label=label.capitalize())\n",
    "    sns.kdeplot(preds[test == 0], fill=True, ax=ax, label=f'No {label.lower()}')\n",
    "    ax.set_xlabel('Predicted probability')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylabel('Density')\n",
    "    # ax.set_title('Density Probability')\n",
    "    ax.legend()\n",
    "    \n",
    "def get_better_ax(ax, axis='y'):\n",
    "    # frame\n",
    "    for spine in ['top', 'right']: \n",
    "        ax.spines[spine].set_visible(False)\n",
    "    ax.tick_params(top=False, right=False)\n",
    "\n",
    "    # y axis\n",
    "    ax.grid(axis=axis, linestyle=\"--\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Nov  6 10:06:52 2018\n",
    "\n",
    "@author: yandexdataschool\n",
    "\n",
    "Original Code found in:\n",
    "https://github.com/yandexdataschool/roc_comparison\n",
    "\n",
    "updated: Raul Sanchez-Vazquez\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float64)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float64)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float64)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float64)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float64)\n",
    "    ty = np.empty([k, n], dtype=np.float64)\n",
    "    tz = np.empty([k, m + n], dtype=np.float64)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float64)\n",
    "    ty = np.empty([k, n], dtype=np.float64)\n",
    "    tz = np.empty([k, m + n], dtype=np.float64)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_with_ci(preds, test, ci=0.95):\n",
    "    auc, auc_cov = delong_roc_variance(test, preds)\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - ci) / 2)\n",
    "    \n",
    "    ci = stats.norm.ppf(lower_upper_q, loc=auc, scale=auc_std)\n",
    "    ci[ci > 1] = 1\n",
    "    return auc, ci[0], ci[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(preds, test, strategy='default'):    \n",
    "    if strategy == 'default':\n",
    "        best_thresh = .5\n",
    "    \n",
    "    elif strategy == 'Youden':\n",
    "        fpr, tpr, thresholds = roc_curve(test, preds)\n",
    "        J = tpr - fpr\n",
    "        ix = np.argmax(J)\n",
    "        best_thresh = thresholds[ix]\n",
    "    \n",
    "    elif strategy == 'F1-score':\n",
    "        precision, recall, thresholds = precision_recall_curve(test, preds)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        ix = np.argmax(fscore)\n",
    "        best_thresh = thresholds[ix]\n",
    "    \n",
    "    elif strategy == '5%-fpr':\n",
    "        best_thresh = 1\n",
    "        fpr=0\n",
    "        while fpr < 5/100:\n",
    "            best_thresh -= .001\n",
    "            preds_binary = np.where(preds >= best_thresh, 1, 0)    \n",
    "            tn, fp, fn, tp = confusion_matrix(test, preds_binary).ravel()\n",
    "            fpr = fp/(fp+tn)\n",
    "    \n",
    "    elif strategy == 'detect half':\n",
    "        best_thresh = 1\n",
    "        detection_rate = 0\n",
    "        while detection_rate < .5:\n",
    "            best_thresh -= .001\n",
    "            preds_binary = np.where(preds >= best_thresh, 1, 0)    \n",
    "            tn, fp, fn, tp = confusion_matrix(test, preds_binary).ravel()\n",
    "            detection_rate = tp/(tp+fn)    \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'The strategy {strategy} is not implemented')\n",
    "    \n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob, title=\"Calibration Plot\", ax=None):\n",
    "      \"\"\"Plot calibration curve and compute calibration metrics\"\"\"\n",
    "      if ax is None:\n",
    "          fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "      # Calibration curve\n",
    "      fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "          y_true, y_prob, n_bins=10, strategy='uniform'\n",
    "      )\n",
    "\n",
    "      # Plot perfect calibration line\n",
    "      ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "\n",
    "      # Plot actual calibration\n",
    "      ax.plot(mean_predicted_value, fraction_of_positives, 's-',\n",
    "              label=f'Model calibration')\n",
    "\n",
    "      ax.set_xlabel('Mean Predicted Probability')\n",
    "      ax.set_ylabel('Fraction of Positives')\n",
    "      ax.set_title(title)\n",
    "      ax.legend()\n",
    "      ax.grid(True, alpha=0.3)\n",
    "\n",
    "      # Compute metrics\n",
    "      brier = brier_score_loss(y_true, y_prob)\n",
    "\n",
    "      # Expected Calibration Error (ECE)\n",
    "      bin_boundaries = np.linspace(0, 1, 11)\n",
    "      bin_lowers = bin_boundaries[:-1]\n",
    "      bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "      ece = 0\n",
    "      for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "          in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "          prop_in_bin = in_bin.mean()\n",
    "\n",
    "          if prop_in_bin > 0:\n",
    "              accuracy_in_bin = y_true[in_bin].mean()\n",
    "              avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "              ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "      ax.text(0.05, 0.95, f'Brier Score: {brier:.3f}\\nECE: {ece:.3f}',\n",
    "              transform=ax.transAxes, verticalalignment='top',\n",
    "              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "      return brier, ece\n",
    "\n",
    "def evaluate(preds, test, title, disease_label):\n",
    "    if preds.ndim == 2: preds = preds[:, 1].reshape(test.shape)\n",
    "    \n",
    "    ## Plot AUC/ROC and density probability\n",
    "    fpr, tpr, thresholds = roc_curve(test, preds)\n",
    "    auc, lower_ci, upper_ci = get_auc_with_ci(preds, test, ci=0.95)\n",
    "    \n",
    "    fig, (ax_roc, ax_density) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plot_roc(fpr, tpr, f'{auc:.2f} ({lower_ci:.2f}-{upper_ci:.2f})', ax_roc)\n",
    "    plot_density(preds, test, ax_density, disease_label)\n",
    "\n",
    "    for ax, axis in [(ax_roc, 'both'), (ax_density, 'y')]: get_better_ax(ax, axis)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    ## Other metrics depending on the threshold\n",
    "    res = {}\n",
    "    for strategy in ['5%-fpr', 'detect half', 'Youden', 'F1-score']:\n",
    "        threshold = find_best_threshold(preds, test, strategy)        \n",
    "        preds_binary = np.where(preds >= threshold, 1, 0)    \n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(test, preds_binary).ravel()\n",
    "        \n",
    "        # Metrics\n",
    "        if fp > 0:\n",
    "            frac = Fraction(f'{tp}/{fp}')\n",
    "            true_to_false_ratio = f'{frac.numerator} to {frac.denominator}'\n",
    "        else:\n",
    "            true_to_false_ratio = 'nan'\n",
    "            \n",
    "        res[strategy] = {\n",
    "            'threshold':threshold,\n",
    "            \n",
    "            'tn':tn,\n",
    "            'fp':fp,\n",
    "            'fn':fn,\n",
    "            'tp':tp,\n",
    "            \n",
    "            'detection_rate':tp/(tp+fn), \n",
    "            'missed_per_10_cases':fn/(tp+fn)*10, \n",
    "            'fpr':fp/(fp+tn), \n",
    "            'true_to_false_ratio':true_to_false_ratio,\n",
    "        }\n",
    "    display(pd.DataFrame(res).T)\n",
    "\n",
    "    # Nouvelles métriques de calibration\n",
    "    print(\"\\n--- Calibration Metrics ---\")\n",
    "    brier, ece = plot_calibration_curve(test, preds,\n",
    "                                         title=f\"Calibration - {title}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Brier Score: {brier:.3f}\")\n",
    "    print(f\"Expected Calibration Error: {ece:.3f}\")\n",
    "    \n",
    "    res['auc']=(auc, lower_ci, upper_ci)\n",
    "    res['brier_score'] = brier\n",
    "    res['ece'] = ece\n",
    "\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08648df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, performances:Dict[str, Dict[str, float]], \n",
    "               disease:str, up_to_year:int, model_name:str, base_dir:str=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Save model and performances in {base_dir}/{disease}_{up_to_year}/{model_name}\n",
    "    respectively as model.pkl and performances.pkl.\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(base_dir, f\"{disease}_{up_to_year}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(model_dir, \"model.pkl\")\n",
    "    with open(model_path, \"wb\") as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    performances_path = os.path.join(model_dir, \"performances.pkl\")\n",
    "    with open(performances_path, \"wb\") as file:\n",
    "        pickle.dump(performances, file)\n",
    "\n",
    "def load_model(disease:str, up_to_year:int, model_name:str, base_dir:str=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Load model and performances saved in {base_dir}/{disease}_{up_to_year}/{model_name}\n",
    "    respectively as model.pkl and performances.pkl.\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(base_dir, f\"{disease}_{up_to_year}\")\n",
    "    with open(path, \"rb\") as file:\n",
    "        model = pickle.load(os.path.join(model_dir, \"model.pkl\"))\n",
    "    \n",
    "    with open(performances_path, \"rb\") as file:\n",
    "        performances = pickle.load(os.path.join(model_dir, \"performances.pkl\"))\n",
    "    \n",
    "    return model, performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d9c57",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9596486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age = 65\n",
    "disease = 'all_dementias'\n",
    "include_charlson_bmi = True\n",
    "pred_up_to_year = 2\n",
    "exclude_mci_baseline = False  \n",
    "include_deceased_as_zeros = True\n",
    "\n",
    "print(f'Task: detect patients that will developp {disease} in the following {pred_up_to_year} years')\n",
    "if exclude_mci_baseline:\n",
    "      print(f'Excluding patients with MCI at baseline')\n",
    "\n",
    "print(f'\\n\\nUK {age}:')\n",
    "X, y = get_data(country='UK', age=age, disease=disease, include_charlson_bmi=include_charlson_bmi, pred_up_to_year=pred_up_to_year,include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "# Exclure les patients MCI à baseline\n",
    "if exclude_mci_baseline and 'mci_at_baseline' in X.columns:\n",
    "    mask = X['mci_at_baseline'] == 0\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    # Supprimer la feature mci_at_baseline\n",
    "    X = X.drop('mci_at_baseline', axis=1)\n",
    "    print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "X_train, X_val, y_train, y_val = split(X, y)\n",
    "\n",
    "print(f'\\n\\nFR {age}:')\n",
    "X_test, y_test = get_data(country='FR', age=age, disease=disease, include_charlson_bmi=include_charlson_bmi, pred_up_to_year=pred_up_to_year)\n",
    "\n",
    "# Exclure les patients MCI à baseline pour le test set aussi\n",
    "if exclude_mci_baseline and 'mci_at_baseline' in X_test.columns:\n",
    "    mask_test = X_test['mci_at_baseline'] == 0\n",
    "    X_test = X_test[mask_test]\n",
    "    y_test = y_test[mask_test]\n",
    "    X_test = X_test.drop('mci_at_baseline', axis=1)\n",
    "    print(f'Removed {(~mask_test).sum()} MCI patients at baseline from test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cab785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "res = {}\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "\n",
    "X_train_sub, X_cal, y_train_sub, y_cal = train_test_split(\n",
    "      X_train, y_train, test_size=0.2, random_state=SEED, stratify=y_train\n",
    "  )\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000).fit(X_train, y_train)\n",
    "model.fit(X_train_sub, y_train_sub)\n",
    "# Calibration post-hoc avec Platt scaling\n",
    "calibrated_clf = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
    "calibrated_clf.fit(X_cal, y_cal)\n",
    "y_prob_calibrated = model.predict_proba(X_val)\n",
    "\n",
    "res['class_weight'] = evaluate(y_prob_calibrated, y_val, f'Results of the baseline on val set using balanced class_weight', 'dementias')['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy = DummyClassifier(strategy='prior')\n",
    "dummy.fit(X_train_sub, y_train_sub)\n",
    "y_prob_dummy = dummy.predict_proba(X_val)\n",
    "brier_dummy = brier_score_loss(y_val, y_prob_dummy[:, 1])\n",
    "print(f\"Brier score dummy: {brier_dummy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6859134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "def compare_roc_curves_mci(age=65, disease='all_dementias', include_charlson_bmi=True, \n",
    "                            pred_up_to_year=5, include_deceased_as_zeros=True):\n",
    "\n",
    "      fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "      # Modèle AVEC patients MCI\n",
    "      print(\"=== AVEC patients MCI ===\")\n",
    "      X_with, y_with = get_data(country='UK', age=age, disease=disease,\n",
    "                                  include_charlson_bmi=include_charlson_bmi,\n",
    "                                  pred_up_to_year=pred_up_to_year,\n",
    "                                  include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "      X_train_with, X_val_with, y_train_with, y_val_with = split(X_with, y_with)\n",
    "      X_train_with = X_train_with.fillna(0)\n",
    "      X_val_with = X_val_with.fillna(0)\n",
    "\n",
    "      model_with = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "      model_with.fit(X_train_with, y_train_with)\n",
    "      y_prob_with = model_with.predict_proba(X_val_with)[:, 1]\n",
    "\n",
    "      fpr_with, tpr_with, _ = roc_curve(y_val_with, y_prob_with)\n",
    "      auc_with, lower_ci_with, upper_ci_with = get_auc_with_ci(y_prob_with, y_val_with, ci=0.95)\n",
    "\n",
    "      # Modèle SANS patients MCI\n",
    "      print(\"=== SANS patients MCI ===\")\n",
    "      X_without, y_without = get_data(country='UK', age=age, disease=disease,\n",
    "                                      include_charlson_bmi=include_charlson_bmi,\n",
    "                                      pred_up_to_year=pred_up_to_year,\n",
    "                                      include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "      # Exclure les patients MCI à baseline\n",
    "      if 'mci_at_baseline' in X_without.columns:\n",
    "          mask = X_without['mci_at_baseline'] == 0\n",
    "          X_without = X_without[mask]\n",
    "          y_without = y_without[mask]\n",
    "          X_without = X_without.drop('mci_at_baseline', axis=1)\n",
    "          print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "      X_train_without, X_val_without, y_train_without, y_val_without = split(X_without, y_without)\n",
    "      X_train_without = X_train_without.fillna(0)\n",
    "      X_val_without = X_val_without.fillna(0)\n",
    "\n",
    "      model_without = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "      model_without.fit(X_train_without, y_train_without)\n",
    "      y_prob_without = model_without.predict_proba(X_val_without)[:, 1]\n",
    "\n",
    "      fpr_without, tpr_without, _ = roc_curve(y_val_without, y_prob_without)\n",
    "      auc_without, lower_ci_without, upper_ci_without = get_auc_with_ci(y_prob_without, y_val_without, ci=0.95)\n",
    "\n",
    "      # Plot des deux courbes avec intervalles de confiance\n",
    "      ax.plot(fpr_with, tpr_with, 'b-', linewidth=2.5,\n",
    "              label=f'MCI patients included (AUC = {auc_with:.2f} [{lower_ci_with:.2f}-{upper_ci_with:.2f}])')\n",
    "      ax.plot(fpr_without, tpr_without, 'r-', linewidth=2.5,\n",
    "              label=f'MCI patients excluded (AUC = {auc_without:.2f} [{lower_ci_without:.2f}-{upper_ci_without:.2f}])')\n",
    "\n",
    "      # Ligne de référence\n",
    "      ax.plot([0, 1], [0, 1], 'k--', alpha=0.6, linewidth=1.5, label='Random classifier')\n",
    "\n",
    "      # Formatage amélioré\n",
    "      ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "      ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "      ax.set_title(f\"All-cause dementia disease risk prediction ROC curve\",\n",
    "                      fontsize=20, pad=20)\n",
    "      ax.legend(loc='lower right', fontsize=16)\n",
    "      ax.grid(True, alpha=0.3)\n",
    "      ax.set_xlim(0, 1)\n",
    "      ax.set_ylim(0, 1)\n",
    "\n",
    "      # Remove top and right spines\n",
    "      ax.spines['top'].set_visible(False)\n",
    "      ax.spines['right'].set_visible(False)\n",
    "\n",
    "      plt.tight_layout()\n",
    "      plt.savefig(OUTPUT_DIR+'figure4.pdf')\n",
    "      plt.show()\n",
    "\n",
    "      # Statistiques comparatives\n",
    "      print(f\"\\n=== RÉSULTATS COMPARATIFS ===\")\n",
    "      print(f\"Avec MCI - Taille dataset: {len(X_with)}, AUC: {auc_with:.2f} [{lower_ci_with:.2f}-{upper_ci_with:.2f}]\")\n",
    "      print(f\"Sans MCI - Taille dataset: {len(X_without)}, AUC: {auc_without:.2f} [{lower_ci_without:.2f}-{upper_ci_without:.2f}]\")\n",
    "      print(f\"Différence AUC: {auc_with - auc_without:.3f}\")\n",
    "\n",
    "      return {\n",
    "          'auc_with_mci': auc_with,\n",
    "          'ci_with_mci': (lower_ci_with, upper_ci_with),\n",
    "          'auc_without_mci': auc_without,\n",
    "          'ci_without_mci': (lower_ci_without, upper_ci_without),\n",
    "          'n_with_mci': len(X_with),\n",
    "          'n_without_mci': len(X_without)\n",
    "      }\n",
    "\n",
    "# Utilisation\n",
    "results = compare_roc_curves_mci(disease='alzheimer', include_deceased_as_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves_multitime(country='UK', age=65, disease='all_dementias', \n",
    "                                     include_charlson_bmi=True, exclude_mci_baseline=False,\n",
    "                                     include_deceased_as_zeros=True, prediction_years=[2, 5, 10]):\n",
    "           \"\"\"\n",
    "           Plot ROC curves for different prediction time horizons (2, 5, 10 years) on the same figure.\n",
    "           \n",
    "           Parameters:\n",
    "           -----------\n",
    "           country : str\n",
    "               'UK' or 'FR'\n",
    "           age : int\n",
    "               65 or 70\n",
    "           disease : str\n",
    "               Disease of interest\n",
    "           include_charlson_bmi : bool\n",
    "               Whether to include BMI and CHARLSON features\n",
    "           exclude_mci_baseline : bool\n",
    "               Whether to exclude patients with MCI at baseline\n",
    "           include_deceased_as_zeros : bool\n",
    "               Whether to include deceased patients as negatives\n",
    "           prediction_years : list\n",
    "               List of prediction horizons in years (e.g., [2, 5, 10])\n",
    "           \"\"\"\n",
    "\n",
    "           fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "           # Colors for different time horizons\n",
    "           colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "           results_summary = {}\n",
    "\n",
    "           for i, pred_year in enumerate(prediction_years):\n",
    "               print(f\"\\n=== PREDICTION À {pred_year} ANS ===\")\n",
    "\n",
    "               # Load data for this prediction horizon\n",
    "               X, y = get_data(country=country, age=age, disease=disease,\n",
    "                              include_charlson_bmi=include_charlson_bmi,\n",
    "                              pred_up_to_year=pred_year,\n",
    "                              include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "               # Exclude MCI patients if requested\n",
    "               if exclude_mci_baseline and 'mci_at_baseline' in X.columns:\n",
    "                   mask = X['mci_at_baseline'] == 0\n",
    "                   X = X[mask]\n",
    "                   y = y[mask]\n",
    "                   X = X.drop('mci_at_baseline', axis=1)\n",
    "                   print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "               # Split data\n",
    "               X_train, X_val, y_train, y_val = split(X, y)\n",
    "               X_train = X_train.fillna(0)\n",
    "               X_val = X_val.fillna(0)\n",
    "\n",
    "               # Train model\n",
    "               model = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "               model.fit(X_train, y_train)\n",
    "\n",
    "               # Get predictions\n",
    "               y_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "               # Calculate ROC curve\n",
    "               fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
    "               auc_score, lower_ci, upper_ci = get_auc_with_ci(y_prob, y_val, ci=0.95)\n",
    "\n",
    "               # Plot ROC curve\n",
    "               color = colors[i % len(colors)]\n",
    "               ax.plot(fpr, tpr, color=color, linewidth=2.5,\n",
    "                      label=f'{pred_year} years (AUC = {auc_score:.2f} [{lower_ci:.2f}-{upper_ci:.2f}])')\n",
    "\n",
    "               # Store results\n",
    "               results_summary[f'{pred_year}_years'] = {\n",
    "                   'auc': auc_score,\n",
    "                   'ci_lower': lower_ci,\n",
    "                   'ci_upper': upper_ci,\n",
    "                   'n_patients': len(X),\n",
    "                   'n_cases': y.sum(),\n",
    "                   'prevalence': y.mean()\n",
    "               }\n",
    "\n",
    "           # Reference line (random classifier)\n",
    "           ax.plot([0, 1], [0, 1], 'k--', alpha=0.6, linewidth=1.5, label='Random classifier')\n",
    "\n",
    "           # Formatting\n",
    "           ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "           ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "           ax.set_title(f\"Alzheimer's disease risk prediction ROC curve\",\n",
    "                       fontsize=20, pad=20)\n",
    "\n",
    "           ax.legend(loc='lower right', fontsize=16)\n",
    "           ax.grid(True, alpha=0.3)\n",
    "           ax.set_xlim(0, 1)\n",
    "           ax.set_ylim(0, 1)\n",
    "\n",
    "           # Remove top and right spines\n",
    "           ax.spines['top'].set_visible(False)\n",
    "           ax.spines['right'].set_visible(False)\n",
    "\n",
    "           plt.tight_layout()\n",
    "           plt.savefig(OUTPUT_DIR+'figure4.pdf')\n",
    "           plt.show()\n",
    "\n",
    "\n",
    "           # Print summary statistics\n",
    "           print(f\"\\n=== RÉSUMÉ COMPARATIF ===\")\n",
    "           print(f\"{'Time':<8} {'AUC':<15} {'95% CI':<20} {'N patients':<12} {'N cases':<10} {'Prevalence':<12}\")\n",
    "           print(\"-\" * 80)\n",
    "\n",
    "           for pred_year in prediction_years:\n",
    "               results = results_summary[f'{pred_year}_years']\n",
    "               print(f\"{pred_year} ans{'':<4} \"\n",
    "                     f\"{results['auc']:.3f}{'':<11} \"\n",
    "                     f\"[{results['ci_lower']:.3f}-{results['ci_upper']:.3f}]{'':<8} \"\n",
    "                     f\"{results['n_patients']:<12,} \"\n",
    "                     f\"{results['n_cases']:<10,} \"\n",
    "                     f\"{results['prevalence']*100:.2f}%\")\n",
    "\n",
    "           return results_summary\n",
    "\n",
    "# Exemple d'utilisation\n",
    "print(\"Comparaison des courbes ROC pour différents horizons de prédiction\")\n",
    "results = plot_roc_curves_multitime(\n",
    "    country='UK',\n",
    "    age=65,\n",
    "    disease='alzheimer',\n",
    "    include_charlson_bmi=True,\n",
    "    exclude_mci_baseline=False,\n",
    "    include_deceased_as_zeros=True,\n",
    "    prediction_years=[2, 5, 10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_table(diseases=['all_dementias', 'alzheimer'], \n",
    "                                      prediction_years=[2, 5, 10], \n",
    "                                      age=65, \n",
    "                                      include_charlson_bmi=True,\n",
    "                                      exclude_mci_baseline=False,\n",
    "                                      include_deceased_as_zeros=True,\n",
    "                                      n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive performance table for different diseases and prediction horizons.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    diseases : list\n",
    "        List of diseases to evaluate\n",
    "    prediction_years : list\n",
    "        List of prediction horizons in years\n",
    "    age : int\n",
    "        Age threshold (65 or 70)\n",
    "    include_charlson_bmi : bool\n",
    "        Whether to include BMI and CHARLSON features\n",
    "    exclude_mci_baseline : bool\n",
    "        Whether to exclude patients with MCI at baseline\n",
    "    include_deceased_as_zeros : bool\n",
    "        Whether to include deceased patients as negatives\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples for confidence intervals\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Performance table with all metrics\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    screening_results = []  # Store data for screening analysis\n",
    "\n",
    "    for disease in diseases:\n",
    "        for pred_year in prediction_years:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing {disease} - {pred_year} years prediction\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            row_data = {\n",
    "                'Disease': 'Dementia' if disease == 'all_dementias' else disease.capitalize(),\n",
    "                'Prediction up to year': f'{pred_year} years'\n",
    "            }\n",
    "\n",
    "            # Evaluate on both UK and FR\n",
    "            for country in ['UK', 'FR']:\n",
    "                print(f\"\\n--- Evaluating on {country} ---\")\n",
    "\n",
    "                try:\n",
    "                    # Load data\n",
    "                    if country == 'UK':\n",
    "                        # For UK: train and test on the same data (with train/val split)\n",
    "                        X, y = get_data(country='UK', age=age, disease=disease,\n",
    "                                        include_charlson_bmi=include_charlson_bmi,\n",
    "                                        pred_up_to_year=pred_year,\n",
    "                                        include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                        # Exclude MCI if requested\n",
    "                        if exclude_mci_baseline and 'mci_at_baseline' in X.columns:\n",
    "                            mask = X['mci_at_baseline'] == 0\n",
    "                            X = X[mask]\n",
    "                            y = y[mask]\n",
    "                            X = X.drop('mci_at_baseline', axis=1)\n",
    "                            print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "                        # Split data\n",
    "                        X_train, X_test, y_train, y_test = split(X, y)\n",
    "                        X_train = X_train.fillna(0)\n",
    "                        X_test = X_test.fillna(0)\n",
    "\n",
    "                    else:  # FR\n",
    "                        # For FR: train on UK, test on FR\n",
    "                        print(\"Training on UK data...\")\n",
    "                        X_train, y_train = get_data(country='UK', age=age, disease=disease,\n",
    "                                                    include_charlson_bmi=include_charlson_bmi,\n",
    "                                                    pred_up_to_year=pred_year,\n",
    "                                                    include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                        if exclude_mci_baseline and 'mci_at_baseline' in X_train.columns:\n",
    "                            mask_train = X_train['mci_at_baseline'] == 0\n",
    "                            X_train = X_train[mask_train]\n",
    "                            y_train = y_train[mask_train]\n",
    "                            X_train = X_train.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                        X_train = X_train.fillna(0)\n",
    "\n",
    "                        print(\"Testing on FR data...\")\n",
    "                        X_test, y_test = get_data(country='FR', age=age, disease=disease,\n",
    "                                                include_charlson_bmi=include_charlson_bmi,\n",
    "                                                pred_up_to_year=pred_year,\n",
    "                                                include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                        if exclude_mci_baseline and 'mci_at_baseline' in X_test.columns:\n",
    "                            mask_test = X_test['mci_at_baseline'] == 0\n",
    "                            X_test = X_test[mask_test]\n",
    "                            y_test = y_test[mask_test]\n",
    "                            X_test = X_test.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                        X_test = X_test.fillna(0)\n",
    "\n",
    "                    print(f\"Train set: {len(X_train)} patients ({y_train.sum()} cases)\")\n",
    "                    print(f\"Test set: {len(X_test)} patients ({y_test.sum()} cases)\")\n",
    "\n",
    "                    # Train model\n",
    "                    model = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    # Get predictions\n",
    "                    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                    # Store data for screening analysis (for ALL diseases on UK validation set)\n",
    "                    if country == 'UK':\n",
    "                        screening_results.append({\n",
    "                            'disease': disease,\n",
    "                            'pred_year': pred_year,\n",
    "                            'y_test': y_test,\n",
    "                            'y_prob': y_prob,\n",
    "                            'n_patients': len(y_test),\n",
    "                            'n_cases': y_test.sum()\n",
    "                        })\n",
    "\n",
    "                    # Calculate ROC AUC with CI\n",
    "                    roc_auc_score, roc_lower_ci, roc_upper_ci = get_auc_with_ci(y_prob, y_test, ci=0.95)\n",
    "                    \n",
    "                    # Calculate Brier score for calibration\n",
    "                    from sklearn.metrics import brier_score_loss\n",
    "                    brier_score = brier_score_loss(y_test, y_prob)\n",
    "\n",
    "                    # Calculate detection rate for 5% FPR\n",
    "                    def bootstrap_metric(y_true, y_scores, metric_func, n_bootstrap=n_bootstrap):\n",
    "                        \"\"\"Bootstrap confidence intervals for custom metrics\"\"\"\n",
    "                        np.random.seed(SEED)\n",
    "                        bootstrap_values = []\n",
    "                        n_samples = len(y_true)\n",
    "\n",
    "                        for _ in range(n_bootstrap):\n",
    "                            # Bootstrap sample\n",
    "                            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                            y_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "                            scores_boot = y_scores[indices]\n",
    "\n",
    "                            try:\n",
    "                                value = metric_func(y_boot, scores_boot)\n",
    "                                if not np.isnan(value):\n",
    "                                    bootstrap_values.append(value)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                        if len(bootstrap_values) > 0:\n",
    "                            return (np.mean(bootstrap_values),\n",
    "                                    np.percentile(bootstrap_values, 2.5),\n",
    "                                    np.percentile(bootstrap_values, 97.5))\n",
    "                        else:\n",
    "                            return (np.nan, np.nan, np.nan)\n",
    "\n",
    "                    # Detection rate at 5% FPR\n",
    "                    def detection_rate_at_5pct_fpr(y_true, y_scores):\n",
    "                        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                        # Find threshold for 5% FPR\n",
    "                        target_fpr = 0.05\n",
    "                        idx = np.argmax(fpr >= target_fpr)\n",
    "                        if idx > 0:\n",
    "                            return tpr[idx] * 100  # Convert to percentage\n",
    "                        return 0\n",
    "\n",
    "                    # FPR at 50% detection rate\n",
    "                    def fpr_at_50pct_detection(y_true, y_scores):\n",
    "                        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                        # Find threshold for 50% TPR\n",
    "                        target_tpr = 0.5\n",
    "                        idx = np.argmax(tpr >= target_tpr)\n",
    "                        if idx < len(fpr):\n",
    "                            return fpr[idx] * 100  # Convert to percentage\n",
    "                        return 100\n",
    "\n",
    "                    # Calculate metrics with bootstrap CIs\n",
    "                    det_rate_mean, det_rate_lower, det_rate_upper = bootstrap_metric(\n",
    "                        y_test, y_prob, detection_rate_at_5pct_fpr)\n",
    "\n",
    "                    fpr_mean, fpr_lower, fpr_upper = bootstrap_metric(\n",
    "                        y_test, y_prob, fpr_at_50pct_detection)\n",
    "\n",
    "                    # Format results\n",
    "                    roc_auc_str = f\"{roc_auc_score:.2f} ({roc_lower_ci:.2f}-{roc_upper_ci:.2f})\"\n",
    "                    brier_str = f\"{brier_score:.3f}\"\n",
    "                    det_rate_str = f\"{det_rate_mean:.1f}% ({det_rate_lower:.1f}%-{det_rate_upper:.1f}%)\"\n",
    "                    fpr_str = f\"{fpr_mean:.1f}% ({fpr_lower:.1f}%-{fpr_upper:.1f}%)\"\n",
    "\n",
    "                    if country == 'UK':\n",
    "                        row_data['ROC AUC on UK'] = roc_auc_str\n",
    "                        row_data['Brier Score on UK'] = brier_str\n",
    "                        row_data['Detection rate for 5% FPR'] = det_rate_str\n",
    "                        row_data['FPR for 50% detection rate'] = fpr_str\n",
    "                        \n",
    "                        # Calculate precision of top 1% and lift based on actual dataset prevalence\n",
    "                        dataset_prevalence = y_test.sum() / len(y_test)  # Actual prevalence in test set\n",
    "                        \n",
    "                        n_patients = len(y_test)\n",
    "                        top_1_percent_size = max(1, int(0.01 * n_patients))  # At least 1 patient\n",
    "                        \n",
    "                        # Sort patients by prediction score (highest first)\n",
    "                        sorted_indices = np.argsort(y_prob)[::-1]\n",
    "                        y_sorted = y_test.iloc[sorted_indices] if hasattr(y_test, 'iloc') else y_test[sorted_indices]\n",
    "                        \n",
    "                        # Get top 1% patients\n",
    "                        top_1_percent_patients = y_sorted[:top_1_percent_size]\n",
    "                        top_1_percent_cases = top_1_percent_patients.sum()\n",
    "                        \n",
    "                        # Calculate precision of top 1%\n",
    "                        precision_top_1_percent = top_1_percent_cases / top_1_percent_size if top_1_percent_size > 0 else 0\n",
    "                        \n",
    "                        # Calculate lift compared to actual dataset prevalence\n",
    "                        lift = precision_top_1_percent / dataset_prevalence if dataset_prevalence > 0 else 0\n",
    "                        \n",
    "                        row_data['Precision top 1%'] = f\"{precision_top_1_percent*100:.1f}%\"\n",
    "                        row_data['Lift (vs dataset prevalence)'] = f\"{lift:.1f}x\"\n",
    "                        \n",
    "                    else:\n",
    "                        row_data['ROC AUC on FR'] = roc_auc_str\n",
    "                        row_data['Brier Score on FR'] = brier_str\n",
    "\n",
    "                    print(f\"{country} Results:\")\n",
    "                    print(f\"  ROC AUC: {roc_auc_str}\")\n",
    "                    print(f\"  Brier Score: {brier_str}\")\n",
    "                    if country == 'UK':\n",
    "                        print(f\"  Detection rate at 5% FPR: {det_rate_str}\")\n",
    "                        print(f\"  FPR at 50% detection: {fpr_str}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {country}: {str(e)}\")\n",
    "                    if country == 'UK':\n",
    "                        row_data['ROC AUC on UK'] = 'N/A'\n",
    "                        row_data['Brier Score on UK'] = 'N/A'\n",
    "                        row_data['Detection rate for 5% FPR'] = 'N/A'\n",
    "                        row_data['FPR for 50% detection rate'] = 'N/A'\n",
    "                        row_data['Precision top 1%'] = 'N/A'\n",
    "                        row_data['Lift (vs dataset prevalence)'] = 'N/A'\n",
    "                    else:\n",
    "                        row_data['ROC AUC on FR'] = 'N/A'\n",
    "                        row_data['Brier Score on FR'] = 'N/A'\n",
    "\n",
    "            results.append(row_data)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Reorder columns to match the desired format\n",
    "    column_order = ['Disease', 'Prediction up to year', 'ROC AUC on UK', 'Brier Score on UK',\n",
    "                    'Detection rate for 5% FPR', 'FPR for 50% detection rate', \n",
    "                    'ROC AUC on FR', 'Brier Score on FR',\n",
    "                    'Precision top 1%', 'Lift (vs dataset prevalence)']\n",
    "    df = df[column_order]\n",
    "\n",
    "    # Calculate screening requirements for 80% detection rate for ALL diseases\n",
    "    if screening_results:  # Only if we have screening data\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SCREENING REQUIREMENTS FOR 80% DETECTION RATE\")\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        for screen_data in screening_results:\n",
    "            disease = screen_data['disease']\n",
    "            pred_year = screen_data['pred_year']\n",
    "            y_test = screen_data['y_test']\n",
    "            y_prob = screen_data['y_prob']\n",
    "            n_patients = screen_data['n_patients']\n",
    "            n_cases = screen_data['n_cases']\n",
    "\n",
    "            # Calculate threshold for 80% detection rate\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "            target_tpr = 0.80\n",
    "\n",
    "            # Find the index where TPR >= 80%\n",
    "            idx = np.argmax(tpr >= target_tpr)\n",
    "\n",
    "            disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "\n",
    "            if idx < len(thresholds) and tpr[idx] >= target_tpr:\n",
    "                threshold_80 = thresholds[idx]\n",
    "                fpr_80 = fpr[idx]\n",
    "                tpr_80 = tpr[idx]\n",
    "\n",
    "                # Calculate how many patients need to be screened\n",
    "                prevalence = n_cases / n_patients\n",
    "\n",
    "                # Number of patients needed to screen per case detected at 80% sensitivity\n",
    "                patients_to_screen_per_case = 1 / (tpr_80 * prevalence) if (tpr_80 * prevalence) > 0 else float('inf')\n",
    "\n",
    "                print(f\"\\n{disease_name} - {pred_year}-year prediction:\")\n",
    "                print(f\"  Threshold for 80% detection: {threshold_80:.3f}\")\n",
    "                print(f\"  Sensitivity (TPR): {tpr_80*100:.1f}%\")\n",
    "                print(f\"  FPR: {fpr_80*100:.1f}%\")\n",
    "                print(f\"  Prevalence in test set: {prevalence*100:.2f}% ({n_cases}/{n_patients})\")\n",
    "                print(f\"  Number needed to screen: {patients_to_screen_per_case:.0f} patients per case detected\")\n",
    "            else:\n",
    "                print(f\"\\n{disease_name} - {pred_year}-year prediction:\")\n",
    "                print(f\"  Cannot achieve 80% detection rate with available data\")\n",
    "                print(f\"  Maximum achievable TPR: {max(tpr)*100:.1f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_algorithm_table(diseases=['all_dementias', 'alzheimer'], \n",
    "                                          prediction_years=[2, 5, 10], \n",
    "                                          age=65, \n",
    "                                          include_charlson_bmi=True,\n",
    "                                          exclude_mci_baseline=False,\n",
    "                                          include_deceased_as_zeros=True,\n",
    "                                          n_bootstrap=500):\n",
    "           \"\"\"\n",
    "           Generate a performance comparison table for different algorithms.\n",
    "           For each prediction task, compare algorithms: Logistic Regression, Random Forest, SVM, Neural Network.\n",
    "           \n",
    "           Parameters:\n",
    "           -----------\n",
    "           diseases : list\n",
    "               List of diseases to evaluate\n",
    "           prediction_years : list\n",
    "               List of prediction horizons in years\n",
    "           age : int\n",
    "               Age threshold (65 or 70)\n",
    "           include_charlson_bmi : bool\n",
    "               Whether to include BMI and CHARLSON features\n",
    "           exclude_mci_baseline : bool\n",
    "               Whether to exclude patients with MCI at baseline\n",
    "           include_deceased_as_zeros : bool\n",
    "               Whether to include deceased patients as negatives\n",
    "           n_bootstrap : int\n",
    "               Number of bootstrap samples for confidence intervals (reduced for speed)\n",
    "               \n",
    "           Returns:\n",
    "           --------\n",
    "           pd.DataFrame\n",
    "               Performance table comparing algorithms\n",
    "           \"\"\"\n",
    "\n",
    "           # Import additional algorithms\n",
    "           from sklearn.ensemble import RandomForestClassifier\n",
    "           from sklearn.svm import SVC\n",
    "           from sklearn.neural_network import MLPClassifier\n",
    "           from sklearn.preprocessing import StandardScaler\n",
    "           from sklearn.pipeline import Pipeline\n",
    "\n",
    "           # Define algorithms to compare\n",
    "           algorithms = {\n",
    "               'Logistic Regression': {\n",
    "                   'model': LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000),\n",
    "                   'name': 'Logistic Regression'\n",
    "               },\n",
    "               'Random Forest': {\n",
    "                   'model': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=SEED, n_jobs=-1),\n",
    "                   'name': 'Random Forest'\n",
    "               },\n",
    "               'SVM': {\n",
    "                   'model': Pipeline([\n",
    "                       ('scaler', StandardScaler()),\n",
    "                       ('svm', SVC(probability=True, class_weight='balanced', random_state=SEED, kernel='rbf'))\n",
    "                   ]),\n",
    "                   'name': 'Support Vector Machine'\n",
    "               },\n",
    "               'Neural Network': {\n",
    "                   'model': Pipeline([\n",
    "                       ('scaler', StandardScaler()),\n",
    "                       ('mlp', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=SEED))\n",
    "                   ]),\n",
    "                   'name': 'Neural Network (MLP)'\n",
    "               }\n",
    "           }\n",
    "\n",
    "           results = []\n",
    "\n",
    "           for disease in diseases:\n",
    "               for pred_year in prediction_years:\n",
    "                   print(f\"\\n{'='*80}\")\n",
    "                   print(f\"Processing {disease} - {pred_year} years prediction\")\n",
    "                   print(f\"{'='*80}\")\n",
    "\n",
    "                   # Load data once for all algorithms\n",
    "                   print(\"Loading UK data...\")\n",
    "                   X_uk, y_uk = get_data(country='UK', age=age, disease=disease,\n",
    "                                        include_charlson_bmi=include_charlson_bmi,\n",
    "                                        pred_up_to_year=pred_year,\n",
    "                                        include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                   print(\"Loading FR data...\")\n",
    "                   X_fr, y_fr = get_data(country='FR', age=age, disease=disease,\n",
    "                                        include_charlson_bmi=include_charlson_bmi,\n",
    "                                        pred_up_to_year=pred_year,\n",
    "                                        include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                   # Exclude MCI if requested\n",
    "                   if exclude_mci_baseline:\n",
    "                       if 'mci_at_baseline' in X_uk.columns:\n",
    "                           mask_uk = X_uk['mci_at_baseline'] == 0\n",
    "                           X_uk = X_uk[mask_uk]\n",
    "                           y_uk = y_uk[mask_uk]\n",
    "                           X_uk = X_uk.drop('mci_at_baseline', axis=1)\n",
    "                           print(f'UK: Removed {(~mask_uk).sum()} MCI patients at baseline')\n",
    "\n",
    "                       if 'mci_at_baseline' in X_fr.columns:\n",
    "                           mask_fr = X_fr['mci_at_baseline'] == 0\n",
    "                           X_fr = X_fr[mask_fr]\n",
    "                           y_fr = y_fr[mask_fr]\n",
    "                           X_fr = X_fr.drop('mci_at_baseline', axis=1)\n",
    "                           print(f'FR: Removed {(~mask_fr).sum()} MCI patients at baseline')\n",
    "\n",
    "                   # Split UK data for training and testing\n",
    "                   X_train, X_test_uk, y_train, y_test_uk = split(X, y)\n",
    "                   X_train = X_train.fillna(0)\n",
    "                   X_test_uk = X_test_uk.fillna(0)\n",
    "                   X_test_fr = X_fr.fillna(0)\n",
    "\n",
    "                   print(f\"Train set: {len(X_train)} patients ({y_train.sum()} cases)\")\n",
    "                   print(f\"UK Test set: {len(X_test_uk)} patients ({y_test_uk.sum()} cases)\")\n",
    "                   print(f\"FR Test set: {len(X_test_fr)} patients ({y_fr.sum()} cases)\")\n",
    "\n",
    "                   # Test each algorithm\n",
    "                   for algo_key, algo_info in algorithms.items():\n",
    "                       print(f\"\\n--- Training {algo_info['name']} ---\")\n",
    "\n",
    "                       row_data = {\n",
    "                           'Disease': 'Dementia' if disease == 'all_dementias' else disease.capitalize(),\n",
    "                           'Prediction up to year': f'{pred_year} years',\n",
    "                           'Algorithm': algo_info['name']\n",
    "                       }\n",
    "\n",
    "                       try:\n",
    "                           # Train model\n",
    "                           model = algo_info['model']\n",
    "\n",
    "                           # Handle class imbalance for Neural Network (no built-in class_weight)\n",
    "                           if 'Neural Network' in algo_key:\n",
    "                               # Calculate class weights manually for MLPClassifier\n",
    "                               from sklearn.utils.class_weight import compute_class_weight\n",
    "                               class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "                               # Create sample weights\n",
    "                               sample_weights = np.array([class_weights[1] if y == 1 else class_weights[0] for y in y_train])\n",
    "\n",
    "                               # Use a custom approach for MLPClassifier by oversampling minority class during training\n",
    "                               from sklearn.utils import resample\n",
    "\n",
    "                               # Separate majority and minority classes\n",
    "                               X_train_majority = X_train[y_train == 0]\n",
    "                               X_train_minority = X_train[y_train == 1]\n",
    "                               y_train_majority = y_train[y_train == 0]\n",
    "                               y_train_minority = y_train[y_train == 1]\n",
    "\n",
    "                               # Upsample minority class to balance\n",
    "                               n_majority = len(X_train_majority)\n",
    "                               n_minority = len(X_train_minority)\n",
    "\n",
    "                               if n_minority < n_majority:\n",
    "                                   # Upsample minority class\n",
    "                                   X_train_minority_upsampled, y_train_minority_upsampled = resample(\n",
    "                                       X_train_minority, y_train_minority,\n",
    "                                       replace=True, n_samples=n_majority, random_state=SEED)\n",
    "\n",
    "                                   # Combine majority class with upsampled minority class\n",
    "                                   X_train_balanced = pd.concat([X_train_majority, X_train_minority_upsampled])\n",
    "                                   y_train_balanced = pd.concat([y_train_majority, y_train_minority_upsampled])\n",
    "\n",
    "                                   # Shuffle the data\n",
    "                                   from sklearn.utils import shuffle\n",
    "                                   X_train_balanced, y_train_balanced = shuffle(X_train_balanced, y_train_balanced, random_state=SEED)\n",
    "                               else:\n",
    "                                   X_train_balanced = X_train\n",
    "                                   y_train_balanced = y_train\n",
    "\n",
    "                               model.fit(X_train_balanced, y_train_balanced)\n",
    "                           else:\n",
    "                               model.fit(X_train, y_train)\n",
    "\n",
    "                           # Bootstrap function for confidence intervals\n",
    "                           def bootstrap_metric_fast(y_true, y_scores, metric_func, n_bootstrap=n_bootstrap):\n",
    "                               \"\"\"Faster bootstrap with reduced samples\"\"\"\n",
    "                               np.random.seed(SEED)\n",
    "                               bootstrap_values = []\n",
    "                               n_samples = len(y_true)\n",
    "\n",
    "                               for _ in range(n_bootstrap):\n",
    "                                   indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                                   y_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "                                   scores_boot = y_scores[indices]\n",
    "\n",
    "                                   try:\n",
    "                                       value = metric_func(y_boot, scores_boot)\n",
    "                                       if not np.isnan(value):\n",
    "                                           bootstrap_values.append(value)\n",
    "                                   except:\n",
    "                                       continue\n",
    "\n",
    "                               if len(bootstrap_values) > 0:\n",
    "                                   return (np.mean(bootstrap_values),\n",
    "                                          np.percentile(bootstrap_values, 2.5),\n",
    "                                          np.percentile(bootstrap_values, 97.5))\n",
    "                               else:\n",
    "                                   return (np.nan, np.nan, np.nan)\n",
    "\n",
    "                           # Define metrics\n",
    "                           def detection_rate_at_5pct_fpr(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               target_fpr = 0.05\n",
    "                               idx = np.argmax(fpr >= target_fpr)\n",
    "                               if idx > 0:\n",
    "                                   return tpr[idx] * 100\n",
    "                               return 0\n",
    "\n",
    "                           def fpr_at_50pct_detection(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               target_tpr = 0.5\n",
    "                               idx = np.argmax(tpr >= target_tpr)\n",
    "                               if idx < len(fpr):\n",
    "                                   return fpr[idx] * 100\n",
    "                               return 100\n",
    "\n",
    "                           # Evaluate on UK test set\n",
    "                           print(\"Evaluating on UK test set...\")\n",
    "                           y_prob_uk = model.predict_proba(X_test_uk)[:, 1]\n",
    "                           auc_uk, lower_ci_uk, upper_ci_uk = get_auc_with_ci(y_prob_uk, y_test_uk, ci=0.95)\n",
    "\n",
    "                           det_rate_uk, det_lower_uk, det_upper_uk = bootstrap_metric_fast(\n",
    "                               y_test_uk, y_prob_uk, detection_rate_at_5pct_fpr)\n",
    "\n",
    "                           fpr_uk, fpr_lower_uk, fpr_upper_uk = bootstrap_metric_fast(\n",
    "                               y_test_uk, y_prob_uk, fpr_at_50pct_detection)\n",
    "\n",
    "                           # Evaluate on FR test set\n",
    "                           print(\"Evaluating on FR test set...\")\n",
    "                           y_prob_fr = model.predict_proba(X_test_fr)[:, 1]\n",
    "                           auc_fr, lower_ci_fr, upper_ci_fr = get_auc_with_ci(y_prob_fr, y_fr, ci=0.95)\n",
    "\n",
    "                           # Format results\n",
    "                           row_data['Test AUC on UK'] = f\"{auc_uk:.2f} ({lower_ci_uk:.2f}-{upper_ci_uk:.2f})\"\n",
    "                           row_data['Detection rate for 5% FPR'] = f\"{det_rate_uk:.1f}% ({det_lower_uk:.1f}%-{det_upper_uk:.1f}%)\"\n",
    "                           row_data['FPR for 50% detection rate'] = f\"{fpr_uk:.1f}% ({fpr_lower_uk:.1f}%-{fpr_upper_uk:.1f}%)\"\n",
    "                           row_data['Test AUC on FR'] = f\"{auc_fr:.2f} ({lower_ci_fr:.2f}-{upper_ci_fr:.2f})\"\n",
    "\n",
    "                           print(f\"  UK AUC: {auc_uk:.3f} ({lower_ci_uk:.3f}-{upper_ci_uk:.3f})\")\n",
    "                           print(f\"  FR AUC: {auc_fr:.3f} ({lower_ci_fr:.3f}-{upper_ci_fr:.3f})\")\n",
    "\n",
    "                       except Exception as e:\n",
    "                           print(f\"Error with {algo_info['name']}: {str(e)}\")\n",
    "                           row_data['Test AUC on UK'] = 'Error'\n",
    "                           row_data['Detection rate for 5% FPR'] = 'Error'\n",
    "                           row_data['FPR for 50% detection rate'] = 'Error'\n",
    "                           row_data['Test AUC on FR'] = 'Error'\n",
    "\n",
    "                       results.append(row_data)\n",
    "\n",
    "           # Create DataFrame\n",
    "           df = pd.DataFrame(results)\n",
    "\n",
    "           # Reorder columns\n",
    "           column_order = ['Disease', 'Prediction up to year', 'Algorithm', 'Test AUC on UK',\n",
    "                          'Detection rate for 5% FPR', 'FPR for 50% detection rate', 'Test AUC on FR']\n",
    "           df = df[column_order]\n",
    "\n",
    "           return df\n",
    "\n",
    "# Generate the multi-algorithm comparison table\n",
    "print(\"Generating multi-algorithm comparison table...\")\n",
    "print(\"This will take several minutes due to multiple model training and bootstrap calculations...\")\n",
    "\n",
    "multi_algo_table = generate_multi_algorithm_table(\n",
    "    diseases=['all_dementias', 'alzheimer'],\n",
    "    prediction_years=[2, 5, 10],\n",
    "    age=65,\n",
    "    include_charlson_bmi=True,\n",
    "    exclude_mci_baseline=False,\n",
    "    include_deceased_as_zeros=True,\n",
    "    n_bootstrap=500  # Reduced for faster computation\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"MULTI-ALGORITHM PERFORMANCE COMPARISON TABLE\")\n",
    "print(\"=\"*120)\n",
    "display(multi_algo_table)\n",
    "\n",
    "# Save the table\n",
    "output_file = os.path.join(OUTPUT_DIR, 'multi_algorithm_performance_table.csv')\n",
    "multi_algo_table.to_csv(output_file, index=False)\n",
    "print(f\"\\nTable saved to: {output_file}\")\n",
    "\n",
    "# Create a summary showing best performing algorithm for each task\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMING ALGORITHM SUMMARY (by UK AUC)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract AUC values for comparison\n",
    "def extract_auc(auc_string):\n",
    "    \"\"\"Extract numeric AUC value from formatted string\"\"\"\n",
    "    try:\n",
    "        return float(auc_string.split(' ')[0])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "multi_algo_table['AUC_numeric'] = multi_algo_table['Test AUC on UK'].apply(extract_auc)\n",
    "\n",
    "# Group by task and find best algorithm\n",
    "summary_results = []\n",
    "for _, group in multi_algo_table.groupby(['Disease', 'Prediction up to year']):\n",
    "    best_row = group.loc[group['AUC_numeric'].idxmax()]\n",
    "    summary_results.append({\n",
    "        'Task': f\"{best_row['Disease']} - {best_row['Prediction up to year']}\",\n",
    "        'Best Algorithm': best_row['Algorithm'],\n",
    "        'UK AUC': best_row['Test AUC on UK'],\n",
    "        'FR AUC': best_row['Test AUC on FR']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "display(summary_df)\n",
    "\n",
    "# Clean up temporary column\n",
    "multi_algo_table = multi_algo_table.drop('AUC_numeric', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_table_screen(diseases=['all_dementias', 'alzheimer'], \n",
    "                                      prediction_years=[2, 5, 10], \n",
    "                                      age=65, \n",
    "                                      include_charlson_bmi=True,\n",
    "                                      exclude_mci_baseline=False,\n",
    "                                      include_deceased_as_zeros=True,\n",
    "                                      n_bootstrap=1000):\n",
    "           \"\"\"\n",
    "           Generate a comprehensive performance table for different diseases and prediction horizons.\n",
    "           \n",
    "           Parameters:\n",
    "           -----------\n",
    "           diseases : list\n",
    "               List of diseases to evaluate\n",
    "           prediction_years : list\n",
    "               List of prediction horizons in years\n",
    "           age : int\n",
    "               Age threshold (65 or 70)\n",
    "           include_charlson_bmi : bool\n",
    "               Whether to include BMI and CHARLSON features\n",
    "           exclude_mci_baseline : bool\n",
    "               Whether to exclude patients with MCI at baseline\n",
    "           include_deceased_as_zeros : bool\n",
    "               Whether to include deceased patients as negatives\n",
    "           n_bootstrap : int\n",
    "               Number of bootstrap samples for confidence intervals\n",
    "               \n",
    "           Returns:\n",
    "           --------\n",
    "           pd.DataFrame\n",
    "               Performance table with all metrics\n",
    "           \"\"\"\n",
    "\n",
    "           results = []\n",
    "           screening_results = []  # Store data for screening analysis\n",
    "\n",
    "           for disease in diseases:\n",
    "               for pred_year in prediction_years:\n",
    "                   print(f\"\\n{'='*60}\")\n",
    "                   print(f\"Processing {disease} - {pred_year} years prediction\")\n",
    "                   print(f\"{'='*60}\")\n",
    "\n",
    "                   row_data = {\n",
    "                       'Disease': 'Dementia' if disease == 'all_dementias' else disease.capitalize(),\n",
    "                       'Prediction up to year': f'{pred_year} years'\n",
    "                   }\n",
    "\n",
    "                   # Evaluate on both UK and FR\n",
    "                   for country in ['UK', 'FR']:\n",
    "                       print(f\"\\n--- Evaluating on {country} ---\")\n",
    "\n",
    "                       try:\n",
    "                           # Load data\n",
    "                           if country == 'UK':\n",
    "                               # For UK: train and test on the same data (with train/val split)\n",
    "                               X, y = get_data(country='UK', age=age, disease=disease,\n",
    "                                              include_charlson_bmi=include_charlson_bmi,\n",
    "                                              pred_up_to_year=pred_year,\n",
    "                                              include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               # Exclude MCI if requested\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X.columns:\n",
    "                                   mask = X['mci_at_baseline'] == 0\n",
    "                                   X = X[mask]\n",
    "                                   y = y[mask]\n",
    "                                   X = X.drop('mci_at_baseline', axis=1)\n",
    "                                   print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "                               # Split data\n",
    "                               X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                   X, y, test_size=0.25, random_state=SEED)\n",
    "                               X_train = X_train.fillna(0)\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           else:  # FR\n",
    "                               # For FR: train on UK, test on FR\n",
    "                               print(\"Training on UK data...\")\n",
    "                               X_train, y_train = get_data(country='UK', age=age, disease=disease,\n",
    "                                                          include_charlson_bmi=include_charlson_bmi,\n",
    "                                                          pred_up_to_year=pred_year,\n",
    "                                                          include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_train.columns:\n",
    "                                   mask_train = X_train['mci_at_baseline'] == 0\n",
    "                                   X_train = X_train[mask_train]\n",
    "                                   y_train = y_train[mask_train]\n",
    "                                   X_train = X_train.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_train = X_train.fillna(0)\n",
    "\n",
    "                               print(\"Testing on FR data...\")\n",
    "                               X_test, y_test = get_data(country='FR', age=age, disease=disease,\n",
    "                                                        include_charlson_bmi=include_charlson_bmi,\n",
    "                                                        pred_up_to_year=pred_year,\n",
    "                                                        include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_test.columns:\n",
    "                                   mask_test = X_test['mci_at_baseline'] == 0\n",
    "                                   X_test = X_test[mask_test]\n",
    "                                   y_test = y_test[mask_test]\n",
    "                                   X_test = X_test.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           print(f\"Train set: {len(X_train)} patients ({y_train.sum()} cases)\")\n",
    "                           print(f\"Test set: {len(X_test)} patients ({y_test.sum()} cases)\")\n",
    "\n",
    "                           # Train model\n",
    "                           model = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "                           model.fit(X_train, y_train)\n",
    "\n",
    "                           # Get predictions\n",
    "                           y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                           # Store data for screening analysis (for ALL diseases on UK validation set)\n",
    "                           if country == 'UK':\n",
    "                               screening_results.append({\n",
    "                                   'disease': disease,\n",
    "                                   'pred_year': pred_year,\n",
    "                                   'y_test': y_test,\n",
    "                                   'y_prob': y_prob,\n",
    "                                   'n_patients': len(X_test),\n",
    "                                   'n_cases': y_test.sum()\n",
    "                               })\n",
    "\n",
    "                           # Calculate AUC with CI\n",
    "                           auc_score, lower_ci, upper_ci = get_auc_with_ci(y_prob, y_test, ci=0.95)\n",
    "\n",
    "                           # Calculate detection rate for 5% FPR\n",
    "                           def bootstrap_metric(y_true, y_scores, metric_func, n_bootstrap=n_bootstrap):\n",
    "                               \"\"\"Bootstrap confidence intervals for custom metrics\"\"\"\n",
    "                               np.random.seed(SEED)\n",
    "                               bootstrap_values = []\n",
    "                               n_samples = len(y_true)\n",
    "\n",
    "                               for _ in range(n_bootstrap):\n",
    "                                   # Bootstrap sample\n",
    "                                   indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                                   y_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "                                   scores_boot = y_scores[indices]\n",
    "\n",
    "                                   try:\n",
    "                                       value = metric_func(y_boot, scores_boot)\n",
    "                                       if not np.isnan(value):\n",
    "                                           bootstrap_values.append(value)\n",
    "                                   except:\n",
    "                                       continue\n",
    "\n",
    "                               if len(bootstrap_values) > 0:\n",
    "                                   return (np.mean(bootstrap_values),\n",
    "                                          np.percentile(bootstrap_values, 2.5),\n",
    "                                          np.percentile(bootstrap_values, 97.5))\n",
    "                               else:\n",
    "                                   return (np.nan, np.nan, np.nan)\n",
    "\n",
    "                           # Detection rate at 5% FPR\n",
    "                           def detection_rate_at_5pct_fpr(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 5% FPR\n",
    "                               target_fpr = 0.05\n",
    "                               idx = np.argmax(fpr >= target_fpr)\n",
    "                               if idx > 0:\n",
    "                                   return tpr[idx] * 100  # Convert to percentage\n",
    "                               return 0\n",
    "\n",
    "                           # FPR at 50% detection rate\n",
    "                           def fpr_at_50pct_detection(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 50% TPR\n",
    "                               target_tpr = 0.5\n",
    "                               idx = np.argmax(tpr >= target_tpr)\n",
    "                               if idx < len(fpr):\n",
    "                                   return fpr[idx] * 100  # Convert to percentage\n",
    "                               return 100\n",
    "\n",
    "                           # Calculate metrics with bootstrap CIs\n",
    "                           det_rate_mean, det_rate_lower, det_rate_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, detection_rate_at_5pct_fpr)\n",
    "\n",
    "                           fpr_mean, fpr_lower, fpr_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, fpr_at_50pct_detection)\n",
    "\n",
    "                           # Format results\n",
    "                           auc_str = f\"{auc_score:.2f} ({lower_ci:.2f}-{upper_ci:.2f})\"\n",
    "                           det_rate_str = f\"{det_rate_mean:.1f}% ({det_rate_lower:.1f}%-{det_rate_upper:.1f}%)\"\n",
    "                           fpr_str = f\"{fpr_mean:.1f}% ({fpr_lower:.1f}%-{fpr_upper:.1f}%)\"\n",
    "\n",
    "                           if country == 'UK':\n",
    "                               row_data['Test AUC on UK'] = auc_str\n",
    "                               row_data['Detection rate for 5% FPR'] = det_rate_str\n",
    "                               row_data['FPR for 50% detection rate'] = fpr_str\n",
    "                           else:\n",
    "                               row_data['Test AUC on FR'] = auc_str\n",
    "\n",
    "                           print(f\"{country} Results:\")\n",
    "                           print(f\"  AUC: {auc_str}\")\n",
    "                           if country == 'UK':\n",
    "                               print(f\"  Detection rate at 5% FPR: {det_rate_str}\")\n",
    "                               print(f\"  FPR at 50% detection: {fpr_str}\")\n",
    "\n",
    "                       except Exception as e:\n",
    "                           print(f\"Error processing {country}: {str(e)}\")\n",
    "                           if country == 'UK':\n",
    "                               row_data['Test AUC on UK'] = 'N/A'\n",
    "                               row_data['Detection rate for 5% FPR'] = 'N/A'\n",
    "                               row_data['FPR for 50% detection rate'] = 'N/A'\n",
    "                           else:\n",
    "                               row_data['Test AUC on FR'] = 'N/A'\n",
    "\n",
    "                   results.append(row_data)\n",
    "\n",
    "           # Create DataFrame\n",
    "           df = pd.DataFrame(results)\n",
    "\n",
    "           # Reorder columns to match the desired format\n",
    "           column_order = ['Disease', 'Prediction up to year', 'Test AUC on UK',\n",
    "                          'Detection rate for 5% FPR', 'FPR for 50% detection rate', 'Test AUC on FR']\n",
    "           df = df[column_order]\n",
    "\n",
    "           # Calculate screening requirements for 80% detection rate for ALL diseases\n",
    "           if screening_results:  # Only if we have screening data\n",
    "               print(\"\\n\" + \"=\"*100)\n",
    "               print(\"SCREENING REQUIREMENTS FOR 80% DETECTION RATE\")\n",
    "               print(\"=\"*100)\n",
    "\n",
    "               for screen_data in screening_results:\n",
    "                   disease = screen_data['disease']\n",
    "                   pred_year = screen_data['pred_year']\n",
    "                   y_test = screen_data['y_test']\n",
    "                   y_prob = screen_data['y_prob']\n",
    "                   n_patients = screen_data['n_patients']\n",
    "                   n_cases = screen_data['n_cases']\n",
    "\n",
    "                   # Calculate threshold for 80% detection rate\n",
    "                   fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "                   target_tpr = 0.80\n",
    "\n",
    "                   # Find the index where TPR >= 80%\n",
    "                   idx = np.argmax(tpr >= target_tpr)\n",
    "\n",
    "                   if idx < len(thresholds) and tpr[idx] >= target_tpr:\n",
    "                       threshold_80 = thresholds[idx]\n",
    "                       fpr_80 = fpr[idx]\n",
    "                       tpr_80 = tpr[idx]\n",
    "\n",
    "                       # Calculate how many patients need to be screened\n",
    "                       # FPR tells us the fraction of negative patients that will test positive\n",
    "                       # TPR tells us the fraction of positive patients that will test positive\n",
    "                       n_negative = n_patients - n_cases\n",
    "                       n_positive = n_cases\n",
    "\n",
    "                       # Expected number of positive tests\n",
    "                       expected_positive_tests = (fpr_80 * n_negative) + (tpr_80 * n_positive)\n",
    "\n",
    "                       # Scale to general population\n",
    "                       prevalence = n_cases / n_patients\n",
    "\n",
    "                       # If we want to detect 80% of patients with disease in a population,\n",
    "                       # we need to screen all patients above the threshold\n",
    "                       patients_to_screen_per_1000 = int(fpr_80 * 1000 + (tpr_80 * prevalence * 1000))\n",
    "                       disease_detected_per_1000 = int(tpr_80 * prevalence * 1000)\n",
    "\n",
    "                       disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "\n",
    "                       print(f\"\\n{pred_year}-year {disease_name} prediction:\")\n",
    "                       print(f\"  Threshold for 80% detection: {threshold_80:.3f}\")\n",
    "                       print(f\"  Sensitivity (TPR): {tpr_80*100:.1f}%\")\n",
    "                       print(f\"  FPR: {fpr_80*100:.1f}%\")\n",
    "                       print(f\"  Prevalence in test set: {prevalence*100:.2f}% ({n_cases}/{n_patients})\")\n",
    "                       print(f\"  \")\n",
    "                       print(f\"  Per 1,000 patients screened:\")\n",
    "                       print(f\"    - Patients testing positive: {patients_to_screen_per_1000}\")\n",
    "                       print(f\"    - {disease_name} patients detected: {disease_detected_per_1000}\")\n",
    "                       print(f\"    - False positives: {patients_to_screen_per_1000 - disease_detected_per_1000}\")\n",
    "                       print(f\"  \")\n",
    "                       print(f\"  Number needed to screen to detect 80% of {disease_name} cases:\")\n",
    "                       if disease_detected_per_1000 > 0:\n",
    "                           nns = int(1000 / disease_detected_per_1000)\n",
    "                           print(f\"    - {nns} patients per {disease_name} case detected\")\n",
    "                       else:\n",
    "                           print(f\"    - Cannot calculate (no cases detected)\")\n",
    "                   else:\n",
    "                       disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "                       print(f\"\\n{pred_year}-year {disease_name} prediction:\")\n",
    "                       print(f\"  Cannot achieve 80% detection rate with available data\")\n",
    "                       print(f\"  Maximum achievable TPR: {max(tpr)*100:.1f}%\")\n",
    "\n",
    "           return df\n",
    "\n",
    "       # Generate the performance table\n",
    "print(\"Generating comprehensive performance table...\")\n",
    "print(\"This may take several minutes due to bootstrap calculations...\")\n",
    "\n",
    "performance_table = generate_performance_table_screen(\n",
    "    diseases=['all_dementias', 'alzheimer'],\n",
    "    prediction_years=[2, 5, 10],\n",
    "    age=65,\n",
    "    include_charlson_bmi=True,\n",
    "    exclude_mci_baseline=False,\n",
    "    include_deceased_as_zeros=True,\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE PERFORMANCE TABLE\")\n",
    "print(\"=\"*100)\n",
    "display(performance_table)\n",
    "\n",
    "# Save the table\n",
    "output_file = os.path.join(OUTPUT_DIR, 'performance_table.csv')\n",
    "performance_table.to_csv(output_file, index=False)\n",
    "print(f\"\\nTable saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_table_screen(diseases=['all_dementias', 'alzheimer'], \n",
    "                                      prediction_years=[2, 5, 10], \n",
    "                                      age=65, \n",
    "                                      include_charlson_bmi=True,\n",
    "                                      exclude_mci_baseline=False,\n",
    "                                      include_deceased_as_zeros=True,\n",
    "                                      n_bootstrap=1000):\n",
    "           \"\"\"\n",
    "           Generate a comprehensive performance table for different diseases and prediction horizons.\n",
    "           \n",
    "           Parameters:\n",
    "           -----------\n",
    "           diseases : list\n",
    "               List of diseases to evaluate\n",
    "           prediction_years : list\n",
    "               List of prediction horizons in years\n",
    "           age : int\n",
    "               Age threshold (65 or 70)\n",
    "           include_charlson_bmi : bool\n",
    "               Whether to include BMI and CHARLSON features\n",
    "           exclude_mci_baseline : bool\n",
    "               Whether to exclude patients with MCI at baseline\n",
    "           include_deceased_as_zeros : bool\n",
    "               Whether to include deceased patients as negatives\n",
    "           n_bootstrap : int\n",
    "               Number of bootstrap samples for confidence intervals\n",
    "               \n",
    "           Returns:\n",
    "           --------\n",
    "           pd.DataFrame\n",
    "               Performance table with all metrics\n",
    "           \"\"\"\n",
    "\n",
    "           results = []\n",
    "           screening_results = []  # Store data for screening analysis\n",
    "\n",
    "           for disease in diseases:\n",
    "               for pred_year in prediction_years:\n",
    "                   print(f\"\\n{'='*60}\")\n",
    "                   print(f\"Processing {disease} - {pred_year} years prediction\")\n",
    "                   print(f\"{'='*60}\")\n",
    "\n",
    "                   row_data = {\n",
    "                       'Disease': 'Dementia' if disease == 'all_dementias' else disease.capitalize(),\n",
    "                       'Prediction up to year': f'{pred_year} years'\n",
    "                   }\n",
    "\n",
    "                   # Evaluate on both UK and FR\n",
    "                   for country in ['UK', 'FR']:\n",
    "                       print(f\"\\n--- Evaluating on {country} ---\")\n",
    "\n",
    "                       try:\n",
    "                           # Load data\n",
    "                           if country == 'UK':\n",
    "                               # For UK: train and test on the same data (with train/val split)\n",
    "                               X, y = get_data(country='UK', age=age, disease=disease,\n",
    "                                              include_charlson_bmi=include_charlson_bmi,\n",
    "                                              pred_up_to_year=pred_year,\n",
    "                                              include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               # Exclude MCI if requested\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X.columns:\n",
    "                                   mask = X['mci_at_baseline'] == 0\n",
    "                                   X = X[mask]\n",
    "                                   y = y[mask]\n",
    "                                   X = X.drop('mci_at_baseline', axis=1)\n",
    "                                   print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "                               # Split data\n",
    "                               X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                   X, y, test_size=0.25, random_state=SEED)\n",
    "                               X_train = X_train.fillna(0)\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           else:  # FR\n",
    "                               # For FR: train on UK, test on FR\n",
    "                               print(\"Training on UK data...\")\n",
    "                               X_train, y_train = get_data(country='UK', age=age, disease=disease,\n",
    "                                                          include_charlson_bmi=include_charlson_bmi,\n",
    "                                                          pred_up_to_year=pred_year,\n",
    "                                                          include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_train.columns:\n",
    "                                   mask_train = X_train['mci_at_baseline'] == 0\n",
    "                                   X_train = X_train[mask_train]\n",
    "                                   y_train = y_train[mask_train]\n",
    "                                   X_train = X_train.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_train = X_train.fillna(0)\n",
    "\n",
    "                               print(\"Testing on FR data...\")\n",
    "                               X_test, y_test = get_data(country='FR', age=age, disease=disease,\n",
    "                                                        include_charlson_bmi=include_charlson_bmi,\n",
    "                                                        pred_up_to_year=pred_year,\n",
    "                                                        include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_test.columns:\n",
    "                                   mask_test = X_test['mci_at_baseline'] == 0\n",
    "                                   X_test = X_test[mask_test]\n",
    "                                   y_test = y_test[mask_test]\n",
    "                                   X_test = X_test.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           print(f\"Train set: {len(X_train)} patients ({y_train.sum()} cases)\")\n",
    "                           print(f\"Test set: {len(X_test)} patients ({y_test.sum()} cases)\")\n",
    "\n",
    "                           # Train model\n",
    "                           model = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "                           model.fit(X_train, y_train)\n",
    "\n",
    "                           # Get predictions\n",
    "                           y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                           # Store data for screening analysis (for ALL diseases on UK validation set)\n",
    "                           if country == 'UK':\n",
    "                               screening_results.append({\n",
    "                                   'disease': disease,\n",
    "                                   'pred_year': pred_year,\n",
    "                                   'y_test': y_test,\n",
    "                                   'y_prob': y_prob,\n",
    "                                   'n_patients': len(X_test),\n",
    "                                   'n_cases': y_test.sum()\n",
    "                               })\n",
    "\n",
    "                           # Calculate AUC with CI\n",
    "                           auc_score, lower_ci, upper_ci = get_auc_with_ci(y_prob, y_test, ci=0.95)\n",
    "\n",
    "                           # Calculate detection rate for 5% FPR\n",
    "                           def bootstrap_metric(y_true, y_scores, metric_func, n_bootstrap=n_bootstrap):\n",
    "                               \"\"\"Bootstrap confidence intervals for custom metrics\"\"\"\n",
    "                               np.random.seed(SEED)\n",
    "                               bootstrap_values = []\n",
    "                               n_samples = len(y_true)\n",
    "\n",
    "                               for _ in range(n_bootstrap):\n",
    "                                   # Bootstrap sample\n",
    "                                   indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                                   y_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "                                   scores_boot = y_scores[indices]\n",
    "\n",
    "                                   try:\n",
    "                                       value = metric_func(y_boot, scores_boot)\n",
    "                                       if not np.isnan(value):\n",
    "                                           bootstrap_values.append(value)\n",
    "                                   except:\n",
    "                                       continue\n",
    "\n",
    "                               if len(bootstrap_values) > 0:\n",
    "                                   return (np.mean(bootstrap_values),\n",
    "                                          np.percentile(bootstrap_values, 2.5),\n",
    "                                          np.percentile(bootstrap_values, 97.5))\n",
    "                               else:\n",
    "                                   return (np.nan, np.nan, np.nan)\n",
    "\n",
    "                           # Detection rate at 5% FPR\n",
    "                           def detection_rate_at_5pct_fpr(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 5% FPR\n",
    "                               target_fpr = 0.05\n",
    "                               idx = np.argmax(fpr >= target_fpr)\n",
    "                               if idx > 0:\n",
    "                                   return tpr[idx] * 100  # Convert to percentage\n",
    "                               return 0\n",
    "\n",
    "                           # FPR at 50% detection rate\n",
    "                           def fpr_at_50pct_detection(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 50% TPR\n",
    "                               target_tpr = 0.5\n",
    "                               idx = np.argmax(tpr >= target_tpr)\n",
    "                               if idx < len(fpr):\n",
    "                                   return fpr[idx] * 100  # Convert to percentage\n",
    "                               return 100\n",
    "\n",
    "                           # Calculate metrics with bootstrap CIs\n",
    "                           det_rate_mean, det_rate_lower, det_rate_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, detection_rate_at_5pct_fpr)\n",
    "\n",
    "                           fpr_mean, fpr_lower, fpr_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, fpr_at_50pct_detection)\n",
    "\n",
    "                           # Format results\n",
    "                           auc_str = f\"{auc_score:.2f} ({lower_ci:.2f}-{upper_ci:.2f})\"\n",
    "                           det_rate_str = f\"{det_rate_mean:.1f}% ({det_rate_lower:.1f}%-{det_rate_upper:.1f}%)\"\n",
    "                           fpr_str = f\"{fpr_mean:.1f}% ({fpr_lower:.1f}%-{fpr_upper:.1f}%)\"\n",
    "\n",
    "                           if country == 'UK':\n",
    "                               row_data['Test AUC on UK'] = auc_str\n",
    "                               row_data['Detection rate for 5% FPR'] = det_rate_str\n",
    "                               row_data['FPR for 50% detection rate'] = fpr_str\n",
    "                           else:\n",
    "                               row_data['Test AUC on FR'] = auc_str\n",
    "\n",
    "                           print(f\"{country} Results:\")\n",
    "                           print(f\"  AUC: {auc_str}\")\n",
    "                           if country == 'UK':\n",
    "                               print(f\"  Detection rate at 5% FPR: {det_rate_str}\")\n",
    "                               print(f\"  FPR at 50% detection: {fpr_str}\")\n",
    "\n",
    "                       except Exception as e:\n",
    "                           print(f\"Error processing {country}: {str(e)}\")\n",
    "                           if country == 'UK':\n",
    "                               row_data['Test AUC on UK'] = 'N/A'\n",
    "                               row_data['Detection rate for 5% FPR'] = 'N/A'\n",
    "                               row_data['FPR for 50% detection rate'] = 'N/A'\n",
    "                           else:\n",
    "                               row_data['Test AUC on FR'] = 'N/A'\n",
    "\n",
    "                   results.append(row_data)\n",
    "\n",
    "           # Create DataFrame\n",
    "           df = pd.DataFrame(results)\n",
    "\n",
    "           # Reorder columns to match the desired format\n",
    "           column_order = ['Disease', 'Prediction up to year', 'Test AUC on UK',\n",
    "                          'Detection rate for 5% FPR', 'FPR for 50% detection rate', 'Test AUC on FR']\n",
    "           df = df[column_order]\n",
    "\n",
    "           # Calculate screening requirements for 80% detection rate for ALL diseases\n",
    "           if screening_results:  # Only if we have screening data\n",
    "               print(\"\\n\" + \"=\"*100)\n",
    "               print(\"SCREENING REQUIREMENTS FOR 80% DETECTION RATE\")\n",
    "               print(\"=\"*100)\n",
    "\n",
    "               for screen_data in screening_results:\n",
    "                   disease = screen_data['disease']\n",
    "                   pred_year = screen_data['pred_year']\n",
    "                   y_test = screen_data['y_test']\n",
    "                   y_prob = screen_data['y_prob']\n",
    "                   n_patients = screen_data['n_patients']\n",
    "                   n_cases = screen_data['n_cases']\n",
    "\n",
    "                   # Calculate threshold for 80% detection rate\n",
    "                   fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "                   target_tpr = 0.80\n",
    "\n",
    "                   # Find the index where TPR >= 80%\n",
    "                   idx = np.argmax(tpr >= target_tpr)\n",
    "\n",
    "                   if idx < len(thresholds) and tpr[idx] >= target_tpr:\n",
    "                       threshold_80 = thresholds[idx]\n",
    "                       fpr_80 = fpr[idx]\n",
    "                       tpr_80 = tpr[idx]\n",
    "\n",
    "                       # Calculate how many patients need to be screened\n",
    "                       # FPR tells us the fraction of negative patients that will test positive\n",
    "                       # TPR tells us the fraction of positive patients that will test positive\n",
    "                       n_negative = n_patients - n_cases\n",
    "                       n_positive = n_cases\n",
    "\n",
    "                       # Expected number of positive tests\n",
    "                       expected_positive_tests = (fpr_80 * n_negative) + (tpr_80 * n_positive)\n",
    "\n",
    "                       # Scale to general population\n",
    "                       prevalence = n_cases / n_patients\n",
    "\n",
    "                       # If we want to detect 80% of patients with disease in a population,\n",
    "                       # we need to screen all patients above the threshold\n",
    "                       patients_to_screen_per_1000 = int(fpr_80 * 1000 + (tpr_80 * prevalence * 1000))\n",
    "                       disease_detected_per_1000 = int(tpr_80 * prevalence * 1000)\n",
    "\n",
    "                       disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "\n",
    "                       print(f\"\\n{pred_year}-year {disease_name} prediction:\")\n",
    "                       print(f\"  Threshold for 80% detection: {threshold_80:.3f}\")\n",
    "                       print(f\"  Sensitivity (TPR): {tpr_80*100:.1f}%\")\n",
    "                       print(f\"  FPR: {fpr_80*100:.1f}%\")\n",
    "                       print(f\"  Prevalence in test set: {prevalence*100:.2f}% ({n_cases}/{n_patients})\")\n",
    "                       print(f\"  \")\n",
    "                       print(f\"  Per 1,000 patients screened:\")\n",
    "                       print(f\"    - Patients testing positive: {patients_to_screen_per_1000}\")\n",
    "                       print(f\"    - {disease_name} patients detected: {disease_detected_per_1000}\")\n",
    "                       print(f\"    - False positives: {patients_to_screen_per_1000 - disease_detected_per_1000}\")\n",
    "                       print(f\"  \")\n",
    "                       print(f\"  Number needed to screen to detect 80% of {disease_name} cases:\")\n",
    "                       if disease_detected_per_1000 > 0:\n",
    "                           nns = int(1000 / disease_detected_per_1000)\n",
    "                           print(f\"    - {nns} patients per {disease_name} case detected\")\n",
    "                       else:\n",
    "                           print(f\"    - Cannot calculate (no cases detected)\")\n",
    "                   else:\n",
    "                       disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "                       print(f\"\\n{pred_year}-year {disease_name} prediction:\")\n",
    "                       print(f\"  Cannot achieve 80% detection rate with available data\")\n",
    "                       print(f\"  Maximum achievable TPR: {max(tpr)*100:.1f}%\")\n",
    "\n",
    "               # NEW ANALYSIS: Top 1% highest scores\n",
    "               print(\"\\n\" + \"=\"*100)\n",
    "               print(\"ANALYSIS: TOP 1% PATIENTS WITH HIGHEST PREDICTION SCORES\")\n",
    "               print(\"=\"*100)\n",
    "\n",
    "               for screen_data in screening_results:\n",
    "                   disease = screen_data['disease']\n",
    "                   pred_year = screen_data['pred_year']\n",
    "                   y_test = screen_data['y_test']\n",
    "                   y_prob = screen_data['y_prob']\n",
    "                   n_patients = screen_data['n_patients']\n",
    "                   n_cases = screen_data['n_cases']\n",
    "\n",
    "                   disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "\n",
    "                   # Sort patients by prediction score (highest first)\n",
    "                   sorted_indices = np.argsort(y_prob)[::-1]\n",
    "                   sorted_y_test = y_test.iloc[sorted_indices] if hasattr(y_test, 'iloc') else y_test[sorted_indices]\n",
    "                   sorted_y_prob = y_prob[sorted_indices]\n",
    "\n",
    "                   # Calculate top 1% threshold\n",
    "                   top_1_percent = max(1, int(0.01 * n_patients))  # At least 1 patient\n",
    "\n",
    "                   # Get the top 1% patients\n",
    "                   top_1_percent_labels = sorted_y_test[:top_1_percent]\n",
    "                   top_1_percent_scores = sorted_y_prob[:top_1_percent]\n",
    "\n",
    "                   # Calculate statistics\n",
    "                   n_disease_in_top1 = top_1_percent_labels.sum()\n",
    "                   precision_top1 = (n_disease_in_top1 / top_1_percent) * 100 if top_1_percent > 0 else 0\n",
    "                   recall_top1 = (n_disease_in_top1 / n_cases) * 100 if n_cases > 0 else 0\n",
    "\n",
    "                   # Calculate enrichment factor (vs random selection)\n",
    "                   overall_prevalence = n_cases / n_patients\n",
    "                   enrichment_factor = (precision_top1 / 100) / overall_prevalence if overall_prevalence > 0 else 0\n",
    "\n",
    "                   print(f\"\\n{pred_year}-year {disease_name} prediction - Top 1% analysis:\")\n",
    "                   print(f\"  Total patients in test set: {n_patients:,}\")\n",
    "                   print(f\"  Total {disease_name.lower()} cases: {n_cases:,} ({overall_prevalence*100:.2f}%)\")\n",
    "                   print(f\"  \")\n",
    "                   print(f\"  Top 1% highest scores ({top_1_percent:,} patients):\")\n",
    "                   print(f\"    - {disease_name} cases found: {n_disease_in_top1}\")\n",
    "                   print(f\"    - Precision (PPV): {precision_top1:.1f}%\")\n",
    "                   print(f\"    - Sensitivity captured: {recall_top1:.1f}%\")\n",
    "                   print(f\"    - Enrichment factor: {enrichment_factor:.1f}x\")\n",
    "                   print(f\"    - Score range: {top_1_percent_scores.min():.3f} - {top_1_percent_scores.max():.3f}\")\n",
    "                   print(f\"  \")\n",
    "                   print(f\"  Clinical interpretation:\")\n",
    "                   if n_disease_in_top1 > 0:\n",
    "                       patients_per_case = top_1_percent / n_disease_in_top1\n",
    "                       print(f\"    - Need to screen {patients_per_case:.1f} high-risk patients to find 1 {disease_name.lower()} case\")\n",
    "                       print(f\"    - In 1000 patients, top 10 would contain ~{(n_disease_in_top1/top_1_percent)*10:.1f} {disease_name.lower()} cases\")\n",
    "                   else:\n",
    "                       print(f\"    - No {disease_name.lower()} cases found in top 1% - model may need improvement\")\n",
    "\n",
    "           return df\n",
    "\n",
    "# Generate the performance table\n",
    "print(\"Generating comprehensive performance table...\")\n",
    "print(\"This may take several minutes due to bootstrap calculations...\")\n",
    "\n",
    "performance_table = generate_performance_table_screen(\n",
    "    diseases=['all_dementias', 'alzheimer'],\n",
    "    prediction_years=[2, 5, 10],\n",
    "    age=65,\n",
    "    include_charlson_bmi=True,\n",
    "    exclude_mci_baseline=False,\n",
    "    include_deceased_as_zeros=True,\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE PERFORMANCE TABLE\")\n",
    "print(\"=\"*100)\n",
    "display(performance_table)\n",
    "\n",
    "# Save the table\n",
    "output_file = os.path.join(OUTPUT_DIR, 'performance_table.csv')\n",
    "performance_table.to_csv(output_file, index=False)\n",
    "print(f\"\\nTable saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_table_screen(diseases=['all_dementias', 'alzheimer'], \n",
    "                                      prediction_years=[2, 5, 10], \n",
    "                                      age=65, \n",
    "                                      include_charlson_bmi=True,\n",
    "                                      exclude_mci_baseline=False,\n",
    "                                      include_deceased_as_zeros=True,\n",
    "                                      n_bootstrap=1000):\n",
    "           \"\"\"\n",
    "           Generate a comprehensive performance table for different diseases and prediction horizons.\n",
    "           \n",
    "           Parameters:\n",
    "           -----------\n",
    "           diseases : list\n",
    "               List of diseases to evaluate\n",
    "           prediction_years : list\n",
    "               List of prediction horizons in years\n",
    "           age : int\n",
    "               Age threshold (65 or 70)\n",
    "           include_charlson_bmi : bool\n",
    "               Whether to include BMI and CHARLSON features\n",
    "           exclude_mci_baseline : bool\n",
    "               Whether to exclude patients with MCI at baseline\n",
    "           include_deceased_as_zeros : bool\n",
    "               Whether to include deceased patients as negatives\n",
    "           n_bootstrap : int\n",
    "               Number of bootstrap samples for confidence intervals\n",
    "               \n",
    "           Returns:\n",
    "           --------\n",
    "           pd.DataFrame\n",
    "               Performance table with all metrics\n",
    "           \"\"\"\n",
    "\n",
    "           results = []\n",
    "           screening_results = []  # Store data for screening analysis\n",
    "\n",
    "           for disease in diseases:\n",
    "               for pred_year in prediction_years:\n",
    "                   print(f\"\\n{'='*60}\")\n",
    "                   print(f\"Processing {disease} - {pred_year} years prediction\")\n",
    "                   print(f\"{'='*60}\")\n",
    "\n",
    "                   row_data = {\n",
    "                       'Disease': 'Dementia' if disease == 'all_dementias' else disease.capitalize(),\n",
    "                       'Prediction up to year': f'{pred_year} years'\n",
    "                   }\n",
    "\n",
    "                   # Evaluate on both UK and FR\n",
    "                   for country in ['UK', 'FR']:\n",
    "                       print(f\"\\n--- Evaluating on {country} ---\")\n",
    "\n",
    "                       try:\n",
    "                           # Load data\n",
    "                           if country == 'UK':\n",
    "                               # For UK: train and test on the same data (with train/val split)\n",
    "                               X, y = get_data(country='UK', age=age, disease=disease,\n",
    "                                              include_charlson_bmi=include_charlson_bmi,\n",
    "                                              pred_up_to_year=pred_year,\n",
    "                                              include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               # Exclude MCI if requested\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X.columns:\n",
    "                                   mask = X['mci_at_baseline'] == 0\n",
    "                                   X = X[mask]\n",
    "                                   y = y[mask]\n",
    "                                   X = X.drop('mci_at_baseline', axis=1)\n",
    "                                   print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "                               # Split data\n",
    "                               X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                   X, y, test_size=0.25, random_state=SEED)\n",
    "                               X_train = X_train.fillna(0)\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           else:  # FR\n",
    "                               # For FR: train on UK, test on FR\n",
    "                               print(\"Training on UK data...\")\n",
    "                               X_train, y_train = get_data(country='UK', age=age, disease=disease,\n",
    "                                                          include_charlson_bmi=include_charlson_bmi,\n",
    "                                                          pred_up_to_year=pred_year,\n",
    "                                                          include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_train.columns:\n",
    "                                   mask_train = X_train['mci_at_baseline'] == 0\n",
    "                                   X_train = X_train[mask_train]\n",
    "                                   y_train = y_train[mask_train]\n",
    "                                   X_train = X_train.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_train = X_train.fillna(0)\n",
    "\n",
    "                               print(\"Testing on FR data...\")\n",
    "                               X_test, y_test = get_data(country='FR', age=age, disease=disease,\n",
    "                                                        include_charlson_bmi=include_charlson_bmi,\n",
    "                                                        pred_up_to_year=pred_year,\n",
    "                                                        include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_test.columns:\n",
    "                                   mask_test = X_test['mci_at_baseline'] == 0\n",
    "                                   X_test = X_test[mask_test]\n",
    "                                   y_test = y_test[mask_test]\n",
    "                                   X_test = X_test.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           print(f\"Train set: {len(X_train)} patients ({y_train.sum()} cases)\")\n",
    "                           print(f\"Test set: {len(X_test)} patients ({y_test.sum()} cases)\")\n",
    "\n",
    "                           # Train model\n",
    "                           model = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "                           model.fit(X_train, y_train)\n",
    "\n",
    "                           # Get predictions\n",
    "                           y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                           # Store data for screening analysis (for ALL diseases on UK validation set)\n",
    "                           if country == 'UK':\n",
    "                               screening_results.append({\n",
    "                                   'disease': disease,\n",
    "                                   'pred_year': pred_year,\n",
    "                                   'y_test': y_test,\n",
    "                                   'y_prob': y_prob,\n",
    "                                   'n_patients': len(X_test),\n",
    "                                   'n_cases': y_test.sum()\n",
    "                               })\n",
    "\n",
    "                           # Calculate AUC with CI\n",
    "                           auc_score, lower_ci, upper_ci = get_auc_with_ci(y_prob, y_test, ci=0.95)\n",
    "\n",
    "                           # Calculate detection rate for 5% FPR\n",
    "                           def bootstrap_metric(y_true, y_scores, metric_func, n_bootstrap=n_bootstrap):\n",
    "                               \"\"\"Bootstrap confidence intervals for custom metrics\"\"\"\n",
    "                               np.random.seed(SEED)\n",
    "                               bootstrap_values = []\n",
    "                               n_samples = len(y_true)\n",
    "\n",
    "                               for _ in range(n_bootstrap):\n",
    "                                   # Bootstrap sample\n",
    "                                   indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                                   y_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "                                   scores_boot = y_scores[indices]\n",
    "\n",
    "                                   try:\n",
    "                                       value = metric_func(y_boot, scores_boot)\n",
    "                                       if not np.isnan(value):\n",
    "                                           bootstrap_values.append(value)\n",
    "                                   except:\n",
    "                                       continue\n",
    "\n",
    "                               if len(bootstrap_values) > 0:\n",
    "                                   return (np.mean(bootstrap_values),\n",
    "                                          np.percentile(bootstrap_values, 2.5),\n",
    "                                          np.percentile(bootstrap_values, 97.5))\n",
    "                               else:\n",
    "                                   return (np.nan, np.nan, np.nan)\n",
    "\n",
    "                           # Detection rate at 5% FPR\n",
    "                           def detection_rate_at_5pct_fpr(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 5% FPR\n",
    "                               target_fpr = 0.05\n",
    "                               idx = np.argmax(fpr >= target_fpr)\n",
    "                               if idx > 0:\n",
    "                                   return tpr[idx] * 100  # Convert to percentage\n",
    "                               return 0\n",
    "\n",
    "                           # FPR at 50% detection rate\n",
    "                           def fpr_at_50pct_detection(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 50% TPR\n",
    "                               target_tpr = 0.5\n",
    "                               idx = np.argmax(tpr >= target_tpr)\n",
    "                               if idx < len(fpr):\n",
    "                                   return fpr[idx] * 100  # Convert to percentage\n",
    "                               return 100\n",
    "\n",
    "                           # Calculate metrics with bootstrap CIs\n",
    "                           det_rate_mean, det_rate_lower, det_rate_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, detection_rate_at_5pct_fpr)\n",
    "\n",
    "                           fpr_mean, fpr_lower, fpr_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, fpr_at_50pct_detection)\n",
    "\n",
    "                           # Format results\n",
    "                           auc_str = f\"{auc_score:.2f} ({lower_ci:.2f}-{upper_ci:.2f})\"\n",
    "                           det_rate_str = f\"{det_rate_mean:.1f}% ({det_rate_lower:.1f}%-{det_rate_upper:.1f}%)\"\n",
    "                           fpr_str = f\"{fpr_mean:.1f}% ({fpr_lower:.1f}%-{fpr_upper:.1f}%)\"\n",
    "\n",
    "                           if country == 'UK':\n",
    "                               row_data['Test AUC on UK'] = auc_str\n",
    "                               row_data['Detection rate for 5% FPR'] = det_rate_str\n",
    "                               row_data['FPR for 50% detection rate'] = fpr_str\n",
    "                           else:\n",
    "                               row_data['Test AUC on FR'] = auc_str\n",
    "\n",
    "                           print(f\"{country} Results:\")\n",
    "                           print(f\"  AUC: {auc_str}\")\n",
    "                           if country == 'UK':\n",
    "                               print(f\"  Detection rate at 5% FPR: {det_rate_str}\")\n",
    "                               print(f\"  FPR at 50% detection: {fpr_str}\")\n",
    "\n",
    "                       except Exception as e:\n",
    "                           print(f\"Error processing {country}: {str(e)}\")\n",
    "                           if country == 'UK':\n",
    "                               row_data['Test AUC on UK'] = 'N/A'\n",
    "                               row_data['Detection rate for 5% FPR'] = 'N/A'\n",
    "                               row_data['FPR for 50% detection rate'] = 'N/A'\n",
    "                           else:\n",
    "                               row_data['Test AUC on FR'] = 'N/A'\n",
    "\n",
    "                   results.append(row_data)\n",
    "\n",
    "           # Create DataFrame\n",
    "           df = pd.DataFrame(results)\n",
    "\n",
    "           # Reorder columns to match the desired format\n",
    "           column_order = ['Disease', 'Prediction up to year', 'Test AUC on UK',\n",
    "                          'Detection rate for 5% FPR', 'FPR for 50% detection rate', 'Test AUC on FR']\n",
    "           df = df[column_order]\n",
    "\n",
    "           # Calculate screening requirements for 80% detection rate for ALL diseases\n",
    "           if screening_results:  # Only if we have screening data\n",
    "               print(\"\\n\" + \"=\"*100)\n",
    "               print(\"SCREENING REQUIREMENTS FOR 80% DETECTION RATE\")\n",
    "               print(\"=\"*100)\n",
    "\n",
    "               for screen_data in screening_results:\n",
    "                   disease = screen_data['disease']\n",
    "                   pred_year = screen_data['pred_year']\n",
    "                   y_test = screen_data['y_test']\n",
    "                   y_prob = screen_data['y_prob']\n",
    "                   n_patients = screen_data['n_patients']\n",
    "                   n_cases = screen_data['n_cases']\n",
    "\n",
    "                   # Calculate threshold for 80% detection rate\n",
    "                   fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "                   target_tpr = 0.80\n",
    "\n",
    "                   # Find the index where TPR >= 80%\n",
    "                   idx = np.argmax(tpr >= target_tpr)\n",
    "\n",
    "                   if idx < len(thresholds) and tpr[idx] >= target_tpr:\n",
    "                       threshold_80 = thresholds[idx]\n",
    "                       fpr_80 = fpr[idx]\n",
    "                       tpr_80 = tpr[idx]\n",
    "\n",
    "                       # Calculate how many patients need to be screened\n",
    "                       # FPR tells us the fraction of negative patients that will test positive\n",
    "                       # TPR tells us the fraction of positive patients that will test positive\n",
    "                       n_negative = n_patients - n_cases\n",
    "                       n_positive = n_cases\n",
    "\n",
    "                       # Expected number of positive tests\n",
    "                       expected_positive_tests = (fpr_80 * n_negative) + (tpr_80 * n_positive)\n",
    "\n",
    "                       # Scale to general population\n",
    "                       prevalence = n_cases / n_patients\n",
    "\n",
    "                       # If we want to detect 80% of patients with disease in a population,\n",
    "                       # we need to screen all patients above the threshold\n",
    "                       patients_to_screen_per_1000 = int(fpr_80 * 1000 + (tpr_80 * prevalence * 1000))\n",
    "                       disease_detected_per_1000 = int(tpr_80 * prevalence * 1000)\n",
    "\n",
    "                       disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "\n",
    "                       print(f\"\\n{pred_year}-year {disease_name} prediction:\")\n",
    "                       print(f\"  Threshold for 80% detection: {threshold_80:.3f}\")\n",
    "                       print(f\"  Sensitivity (TPR): {tpr_80*100:.1f}%\")\n",
    "                       print(f\"  FPR: {fpr_80*100:.1f}%\")\n",
    "                       print(f\"  Prevalence in test set: {prevalence*100:.2f}% ({n_cases}/{n_patients})\")\n",
    "                       print(f\"  \")\n",
    "                       print(f\"  Per 1,000 patients screened:\")\n",
    "                       print(f\"    - Patients testing positive: {patients_to_screen_per_1000}\")\n",
    "                       print(f\"    - {disease_name} patients detected: {disease_detected_per_1000}\")\n",
    "                       print(f\"    - False positives: {patients_to_screen_per_1000 - disease_detected_per_1000}\")\n",
    "                       print(f\"  \")\n",
    "                       print(f\"  Number needed to screen to detect 80% of {disease_name} cases:\")\n",
    "                       if disease_detected_per_1000 > 0:\n",
    "                           nns = int(1000 / disease_detected_per_1000)\n",
    "                           print(f\"    - {nns} patients per {disease_name} case detected\")\n",
    "                       else:\n",
    "                           print(f\"    - Cannot calculate (no cases detected)\")\n",
    "                   else:\n",
    "                       disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "                       print(f\"\\n{pred_year}-year {disease_name} prediction:\")\n",
    "                       print(f\"  Cannot achieve 80% detection rate with available data\")\n",
    "                       print(f\"  Maximum achievable TPR: {max(tpr)*100:.1f}%\")\n",
    "\n",
    "               # ENRICHMENT ANALYSIS: Algorithm vs Random Selection for Blood Test Decision\n",
    "               print(\"\\n\" + \"=\"*120)\n",
    "               print(\"BLOOD TEST ENRICHMENT ANALYSIS: Algorithm vs Random Selection\")\n",
    "               print(\"Scenario: GP selects 1% of patients for expensive blood test\")\n",
    "               print(\"=\"*120)\n",
    "\n",
    "               enrichment_table_data = []\n",
    "\n",
    "               for screen_data in screening_results:\n",
    "                   disease = screen_data['disease']\n",
    "                   pred_year = screen_data['pred_year']\n",
    "                   y_test = screen_data['y_test']\n",
    "                   y_prob = screen_data['y_prob']\n",
    "                   n_patients = screen_data['n_patients']\n",
    "                   n_cases = screen_data['n_cases']\n",
    "\n",
    "                   disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "\n",
    "                   # Overall prevalence in the population\n",
    "                   overall_prevalence = n_cases / n_patients\n",
    "\n",
    "                   # Calculate top 1% threshold\n",
    "                   top_1_percent = max(1, int(0.01 * n_patients))  # At least 1 patient\n",
    "\n",
    "                   # SCENARIO 1: Random selection (baseline)\n",
    "                   # If GP randomly selects 1% for blood test\n",
    "                   random_cases_expected = overall_prevalence * top_1_percent\n",
    "\n",
    "                   # SCENARIO 2: Algorithm-guided selection\n",
    "                   # Sort patients by prediction score (highest first)\n",
    "                   sorted_indices = np.argsort(y_prob)[::-1]\n",
    "                   sorted_y_test = y_test.iloc[sorted_indices] if hasattr(y_test, 'iloc') else y_test[sorted_indices]\n",
    "                   sorted_y_prob = y_prob[sorted_indices]\n",
    "\n",
    "                   # Get the top 1% patients according to algorithm\n",
    "                   top_1_percent_labels = sorted_y_test[:top_1_percent]\n",
    "                   top_1_percent_scores = sorted_y_prob[:top_1_percent]\n",
    "\n",
    "                   # Calculate actual cases found\n",
    "                   algorithm_cases_found = top_1_percent_labels.sum()\n",
    "\n",
    "                   # Calculate enrichment factor\n",
    "                   enrichment_factor = algorithm_cases_found / random_cases_expected if random_cases_expected > 0 else 0\n",
    "\n",
    "                   # Calculate precision for both scenarios\n",
    "                   random_precision = (random_cases_expected / top_1_percent) * 100\n",
    "                   algorithm_precision = (algorithm_cases_found / top_1_percent) * 100\n",
    "\n",
    "                   # Calculate cost-effectiveness metrics\n",
    "                   cost_per_case_random = top_1_percent / random_cases_expected if random_cases_expected > 0 else float('inf')\n",
    "                   cost_per_case_algorithm = top_1_percent / algorithm_cases_found if algorithm_cases_found > 0 else float('inf')\n",
    "                   cost_reduction = ((cost_per_case_random - cost_per_case_algorithm) / cost_per_case_random) * 100 if cost_per_case_random > 0 else 0\n",
    "\n",
    "                   # Store results for table\n",
    "                   enrichment_table_data.append({\n",
    "                       'Disease': disease_name,\n",
    "                       'Prediction Horizon': f'{pred_year} years',\n",
    "                       'Population Prevalence': f'{overall_prevalence*100:.2f}%',\n",
    "                       'Random Selection (1%)': f'{random_cases_expected:.1f} cases',\n",
    "                       'Algorithm Selection (1%)': f'{algorithm_cases_found} cases',\n",
    "                       'Enrichment Factor': f'{enrichment_factor:.1f}x',\n",
    "                       'Random Precision': f'{random_precision:.1f}%',\n",
    "                       'Algorithm Precision': f'{algorithm_precision:.1f}%',\n",
    "                       'Blood Tests per Case (Random)': f'{cost_per_case_random:.1f}',\n",
    "                       'Blood Tests per Case (Algorithm)': f'{cost_per_case_algorithm:.1f}',\n",
    "                       'Cost Reduction': f'{cost_reduction:.1f}%'\n",
    "                   })\n",
    "\n",
    "                   print(f\"\\n{pred_year}-year {disease_name} prediction:\")\n",
    "                   print(f\"  Population: {n_patients:,} patients, {n_cases} cases ({overall_prevalence*100:.2f}% prevalence)\")\n",
    "                   print(f\"  Blood test budget: 1% = {top_1_percent} tests\")\n",
    "                   print(f\"  \")\n",
    "                   print(f\"  RANDOM SELECTION (baseline):\")\n",
    "                   print(f\"    - Expected cases found: {random_cases_expected:.1f}\")\n",
    "                   print(f\"    - Precision: {random_precision:.1f}%\")\n",
    "                   print(f\"    - Blood tests per case: {cost_per_case_random:.1f}\")\n",
    "                   print(f\"  \")\n",
    "                   print(f\"  ALGORITHM SELECTION:\")\n",
    "                   print(f\"    - Actual cases found: {algorithm_cases_found}\")\n",
    "                   print(f\"    - Precision: {algorithm_precision:.1f}%\")\n",
    "                   print(f\"    - Blood tests per case: {cost_per_case_algorithm:.1f}\")\n",
    "                   print(f\"  \")\n",
    "                   print(f\"  IMPROVEMENT:\")\n",
    "                   print(f\"    - Enrichment factor: {enrichment_factor:.1f}x\")\n",
    "                   print(f\"    - Cost reduction: {cost_reduction:.1f}%\")\n",
    "                   print(f\"    - Additional cases found: {algorithm_cases_found - random_cases_expected:.1f}\")\n",
    "\n",
    "               # Create enrichment comparison table\n",
    "               enrichment_df = pd.DataFrame(enrichment_table_data)\n",
    "\n",
    "               print(f\"\\n{'='*150}\")\n",
    "               print(\"BLOOD TEST ENRICHMENT COMPARISON TABLE\")\n",
    "               print(f\"{'='*150}\")\n",
    "               display(enrichment_df)\n",
    "\n",
    "               # Save enrichment table\n",
    "               enrichment_output_file = os.path.join(OUTPUT_DIR, 'blood_test_enrichment_table.csv')\n",
    "               enrichment_df_transposed = enrichment_df.T\n",
    "               enrichment_df_transposed.reset_index(inplace=True)\n",
    "               enrichment_df_transposed.rename(columns={'index': 'Metric'}, inplace=True)\n",
    "               enrichment_df_transposed.to_csv(enrichment_output_file, index=False)\n",
    "               print(f\"\\nEnrichment table saved to: {enrichment_output_file}\")\n",
    "\n",
    "               # Summary statistics\n",
    "               print(f\"\\n{'='*100}\")\n",
    "               print(\"SUMMARY: Blood Test Cost-Effectiveness\")\n",
    "               print(f\"{'='*100}\")\n",
    "\n",
    "               avg_enrichment = enrichment_df['Enrichment Factor'].str.replace('x', '').astype(float).mean()\n",
    "               avg_cost_reduction = enrichment_df['Cost Reduction'].str.replace('%', '').astype(float).mean()\n",
    "\n",
    "               print(f\"Average enrichment factor across all tasks: {avg_enrichment:.1f}x\")\n",
    "               print(f\"Average cost reduction: {avg_cost_reduction:.1f}%\")\n",
    "               print(f\"\")\n",
    "               print(f\"Clinical Impact:\")\n",
    "               print(f\"  - Algorithm identifies {avg_enrichment:.1f}x more cases than random selection\")\n",
    "               print(f\"  - Reduces blood test costs by {avg_cost_reduction:.1f}% for same detection rate\")\n",
    "               print(f\"  - Enables precision medicine approach for expensive diagnostic tests\")\n",
    "\n",
    "           return df\n",
    "\n",
    "# Generate the performance table\n",
    "print(\"Generating comprehensive performance table...\")\n",
    "print(\"This will take several minutes due to bootstrap calculations...\")\n",
    "\n",
    "performance_table = generate_performance_table_screen(\n",
    "    diseases=['all_dementias', 'alzheimer'],\n",
    "    prediction_years=[2, 5, 10],\n",
    "    age=65,\n",
    "    include_charlson_bmi=True,\n",
    "    exclude_mci_baseline=False,\n",
    "    include_deceased_as_zeros=True,\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE PERFORMANCE TABLE\")\n",
    "print(\"=\"*100)\n",
    "display(performance_table)\n",
    "\n",
    "# Save the table\n",
    "output_file = os.path.join(OUTPUT_DIR, 'performance_table.csv')\n",
    "performance_table.to_csv(output_file, index=False)\n",
    "print(f\"\\nTable saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_table(diseases=['all_dementias', 'alzheimer'], \n",
    "                                             prediction_years=[2, 5, 10], \n",
    "                                             age=65, \n",
    "                                             include_charlson_bmi=True,\n",
    "                                             exclude_mci_baseline=False,\n",
    "                                             include_deceased_as_zeros=True,\n",
    "                                             n_bootstrap=1000):\n",
    "           \"\"\"\n",
    "           Generate a comprehensive performance table for different diseases and prediction horizons.\n",
    "           \n",
    "           Parameters:\n",
    "           -----------\n",
    "           diseases : list\n",
    "               List of diseases to evaluate\n",
    "           prediction_years : list\n",
    "               List of prediction horizons in years\n",
    "           age : int\n",
    "               Age threshold (65 or 70)\n",
    "           include_charlson_bmi : bool\n",
    "               Whether to include BMI and CHARLSON features\n",
    "           exclude_mci_baseline : bool\n",
    "               Whether to exclude patients with MCI at baseline\n",
    "           include_deceased_as_zeros : bool\n",
    "               Whether to include deceased patients as negatives\n",
    "           n_bootstrap : int\n",
    "               Number of bootstrap samples for confidence intervals\n",
    "               \n",
    "           Returns:\n",
    "           --------\n",
    "           pd.DataFrame\n",
    "               Performance table with all metrics\n",
    "           \"\"\"\n",
    "\n",
    "           results = []\n",
    "           screening_results = []  # Store data for screening analysis\n",
    "\n",
    "           for disease in diseases:\n",
    "               for pred_year in prediction_years:\n",
    "                   print(f\"\\n{'='*60}\")\n",
    "                   print(f\"Processing {disease} - {pred_year} years prediction\")\n",
    "                   print(f\"{'='*60}\")\n",
    "\n",
    "                   row_data = {\n",
    "                       'Disease': 'Dementia' if disease == 'all_dementias' else disease.capitalize(),\n",
    "                       'Prediction up to year': f'{pred_year} years'\n",
    "                   }\n",
    "\n",
    "                   # Evaluate on both UK and FR\n",
    "                   for country in ['UK', 'FR']:\n",
    "                       print(f\"\\n--- Evaluating on {country} ---\")\n",
    "\n",
    "                       try:\n",
    "                           # Load data\n",
    "                           if country == 'UK':\n",
    "                               # For UK: train and test on the same data (with train/val split)\n",
    "                               X, y = get_data(country='UK', age=age, disease=disease,\n",
    "                                               include_charlson_bmi=include_charlson_bmi,\n",
    "                                               pred_up_to_year=pred_year,\n",
    "                                               include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               # Exclude MCI if requested\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X.columns:\n",
    "                                   mask = X['mci_at_baseline'] == 0\n",
    "                                   X = X[mask]\n",
    "                                   y = y[mask]\n",
    "                                   X = X.drop('mci_at_baseline', axis=1)\n",
    "                                   print(f'Removed {(~mask).sum()} MCI patients at baseline')\n",
    "\n",
    "                               # Split data\n",
    "                               X_train, X_test, y_train, y_test = split(X, y)\n",
    "                               X_train = X_train.fillna(0)\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           else:  # FR\n",
    "                               # For FR: train on UK, test on FR\n",
    "                               print(\"Training on UK data...\")\n",
    "                               X_train, y_train = get_data(country='UK', age=age, disease=disease,\n",
    "                                                           include_charlson_bmi=include_charlson_bmi,\n",
    "                                                           pred_up_to_year=pred_year,\n",
    "                                                           include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_train.columns:\n",
    "                                   mask_train = X_train['mci_at_baseline'] == 0\n",
    "                                   X_train = X_train[mask_train]\n",
    "                                   y_train = y_train[mask_train]\n",
    "                                   X_train = X_train.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_train = X_train.fillna(0)\n",
    "\n",
    "                               print(\"Testing on FR data...\")\n",
    "                               X_test, y_test = get_data(country='FR', age=age, disease=disease,\n",
    "                                                       include_charlson_bmi=include_charlson_bmi,\n",
    "                                                       pred_up_to_year=pred_year,\n",
    "                                                       include_deceased_as_zeros=include_deceased_as_zeros)\n",
    "\n",
    "                               if exclude_mci_baseline and 'mci_at_baseline' in X_test.columns:\n",
    "                                   mask_test = X_test['mci_at_baseline'] == 0\n",
    "                                   X_test = X_test[mask_test]\n",
    "                                   y_test = y_test[mask_test]\n",
    "                                   X_test = X_test.drop('mci_at_baseline', axis=1)\n",
    "\n",
    "                               X_test = X_test.fillna(0)\n",
    "\n",
    "                           print(f\"Train set: {len(X_train)} patients ({y_train.sum()} cases)\")\n",
    "                           print(f\"Test set: {len(X_test)} patients ({y_test.sum()} cases)\")\n",
    "\n",
    "                           # Train model\n",
    "                           model = LogisticRegression(class_weight='balanced', random_state=SEED, max_iter=1_000)\n",
    "                           model.fit(X_train, y_train)\n",
    "\n",
    "                           # Get predictions\n",
    "                           y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                           # Store data for screening analysis (for ALL diseases on UK validation set)\n",
    "                           if country == 'UK':\n",
    "                               screening_results.append({\n",
    "                                   'disease': disease,\n",
    "                                   'pred_year': pred_year,\n",
    "                                   'y_test': y_test,\n",
    "                                   'y_prob': y_prob,\n",
    "                                   'n_patients': len(y_test),\n",
    "                                   'n_cases': y_test.sum()\n",
    "                               })\n",
    "\n",
    "                           # Calculate ROC AUC with CI\n",
    "                           roc_auc_score, roc_lower_ci, roc_upper_ci = get_auc_with_ci(y_prob, y_test, ci=0.95)\n",
    "\n",
    "                           # Calculate Brier score for calibration\n",
    "                           from sklearn.metrics import brier_score_loss\n",
    "                           brier_score = brier_score_loss(y_test, y_prob)\n",
    "\n",
    "                           # Calculate detection rate for 5% FPR\n",
    "                           def bootstrap_metric(y_true, y_scores, metric_func, n_bootstrap=n_bootstrap):\n",
    "                               \"\"\"Bootstrap confidence intervals for custom metrics\"\"\"\n",
    "                               np.random.seed(SEED)\n",
    "                               bootstrap_values = []\n",
    "                               n_samples = len(y_true)\n",
    "\n",
    "                               for _ in range(n_bootstrap):\n",
    "                                   # Bootstrap sample\n",
    "                                   indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                                   y_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "                                   scores_boot = y_scores[indices]\n",
    "\n",
    "                                   try:\n",
    "                                       value = metric_func(y_boot, scores_boot)\n",
    "                                       if not np.isnan(value):\n",
    "                                           bootstrap_values.append(value)\n",
    "                                   except:\n",
    "                                       continue\n",
    "\n",
    "                               if len(bootstrap_values) > 0:\n",
    "                                   return (np.mean(bootstrap_values),\n",
    "                                           np.percentile(bootstrap_values, 2.5),\n",
    "                                           np.percentile(bootstrap_values, 97.5))\n",
    "                               else:\n",
    "                                   return (np.nan, np.nan, np.nan)\n",
    "\n",
    "                           # Detection rate at 5% FPR\n",
    "                           def detection_rate_at_5pct_fpr(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 5% FPR\n",
    "                               target_fpr = 0.05\n",
    "                               idx = np.argmax(fpr >= target_fpr)\n",
    "                               if idx > 0:\n",
    "                                   return tpr[idx] * 100  # Convert to percentage\n",
    "                               return 0\n",
    "\n",
    "                           # FPR at 50% detection rate\n",
    "                           def fpr_at_50pct_detection(y_true, y_scores):\n",
    "                               fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                               # Find threshold for 50% TPR\n",
    "                               target_tpr = 0.5\n",
    "                               idx = np.argmax(tpr >= target_tpr)\n",
    "                               if idx < len(fpr):\n",
    "                                   return fpr[idx] * 100  # Convert to percentage\n",
    "                               return 100\n",
    "\n",
    "                           # Calculate metrics with bootstrap CIs\n",
    "                           det_rate_mean, det_rate_lower, det_rate_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, detection_rate_at_5pct_fpr)\n",
    "\n",
    "                           fpr_mean, fpr_lower, fpr_upper = bootstrap_metric(\n",
    "                               y_test, y_prob, fpr_at_50pct_detection)\n",
    "\n",
    "                           # Format results\n",
    "                           roc_auc_str = f\"{roc_auc_score:.2f} ({roc_lower_ci:.2f}-{roc_upper_ci:.2f})\"\n",
    "                           brier_str = f\"{brier_score:.3f}\"\n",
    "                           det_rate_str = f\"{det_rate_mean:.1f}% ({det_rate_lower:.1f}%-{det_rate_upper:.1f}%)\"\n",
    "                           fpr_str = f\"{fpr_mean:.1f}% ({fpr_lower:.1f}%-{fpr_upper:.1f}%)\"\n",
    "\n",
    "                           if country == 'UK':\n",
    "                               row_data['ROC AUC on UK'] = roc_auc_str\n",
    "                               row_data['Brier Score on UK'] = brier_str\n",
    "                               row_data['Detection rate for 5% FPR'] = det_rate_str\n",
    "                               row_data['FPR for 50% detection rate'] = fpr_str\n",
    "\n",
    "                               # Calculate precision of top 1% and lift based on actual dataset prevalence\n",
    "                               dataset_prevalence = y_test.sum() / len(y_test)  # Actual prevalence in test set\n",
    "\n",
    "                               n_patients = len(y_test)\n",
    "                               top_1_percent_size = max(1, int(0.01 * n_patients))  # At least 1 patient\n",
    "\n",
    "                               # Sort patients by prediction score (highest first)\n",
    "                               sorted_indices = np.argsort(y_prob)[::-1]\n",
    "                               y_sorted = y_test.iloc[sorted_indices] if hasattr(y_test, 'iloc') else y_test[sorted_indices]\n",
    "\n",
    "                               # Get top 1% patients\n",
    "                               top_1_percent_patients = y_sorted[:top_1_percent_size]\n",
    "                               top_1_percent_cases = top_1_percent_patients.sum()\n",
    "\n",
    "                               # Calculate precision of top 1%\n",
    "                               precision_top_1_percent = top_1_percent_cases / top_1_percent_size if top_1_percent_size > 0 else 0\n",
    "\n",
    "                               # Calculate lift compared to actual dataset prevalence\n",
    "                               lift = precision_top_1_percent / dataset_prevalence if dataset_prevalence > 0 else 0\n",
    "\n",
    "                               row_data['Precision top 1%'] = f\"{precision_top_1_percent*100:.1f}%\"\n",
    "                               row_data['Lift (vs dataset prevalence)'] = f\"{lift:.1f}x\"\n",
    "\n",
    "                           else:\n",
    "                               row_data['ROC AUC on FR'] = roc_auc_str\n",
    "                               row_data['Brier Score on FR'] = brier_str\n",
    "\n",
    "                           print(f\"{country} Results:\")\n",
    "                           print(f\"  ROC AUC: {roc_auc_str}\")\n",
    "                           print(f\"  Brier Score: {brier_str}\")\n",
    "                           if country == 'UK':\n",
    "                               print(f\"  Detection rate at 5% FPR: {det_rate_str}\")\n",
    "                               print(f\"  FPR at 50% detection: {fpr_str}\")\n",
    "\n",
    "                       except Exception as e:\n",
    "                           print(f\"Error processing {country}: {str(e)}\")\n",
    "                           if country == 'UK':\n",
    "                               row_data['ROC AUC on UK'] = 'N/A'\n",
    "                               row_data['Brier Score on UK'] = 'N/A'\n",
    "                               row_data['Detection rate for 5% FPR'] = 'N/A'\n",
    "                               row_data['FPR for 50% detection rate'] = 'N/A'\n",
    "                               row_data['Precision top 1%'] = 'N/A'\n",
    "                               row_data['Lift (vs dataset prevalence)'] = 'N/A'\n",
    "                           else:\n",
    "                               row_data['ROC AUC on FR'] = 'N/A'\n",
    "                               row_data['Brier Score on FR'] = 'N/A'\n",
    "\n",
    "                   results.append(row_data)\n",
    "\n",
    "           # Create DataFrame\n",
    "           df = pd.DataFrame(results)\n",
    "\n",
    "           # Reorder columns to match the desired format\n",
    "           column_order = ['Disease', 'Prediction up to year', 'ROC AUC on UK', 'Brier Score on UK',\n",
    "                           'Detection rate for 5% FPR', 'FPR for 50% detection rate',\n",
    "                           'ROC AUC on FR', 'Brier Score on FR',\n",
    "                           'Precision top 1%', 'Lift (vs dataset prevalence)']\n",
    "           df = df[column_order]\n",
    "\n",
    "           # Calculate screening requirements for 80% detection rate for ALL diseases\n",
    "           if screening_results:  # Only if we have screening data\n",
    "               print(\"\\n\" + \"=\"*100)\n",
    "               print(\"SCREENING REQUIREMENTS FOR 80% DETECTION RATE\")\n",
    "               print(\"=\"*100)\n",
    "\n",
    "               for screen_data in screening_results:\n",
    "                   disease = screen_data['disease']\n",
    "                   pred_year = screen_data['pred_year']\n",
    "                   y_test = screen_data['y_test']\n",
    "                   y_prob = screen_data['y_prob']\n",
    "                   n_patients = screen_data['n_patients']\n",
    "                   n_cases = screen_data['n_cases']\n",
    "\n",
    "                   # Calculate threshold for 80% detection rate\n",
    "                   fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "                   target_tpr = 0.80\n",
    "\n",
    "                   # Find the index where TPR >= 80%\n",
    "                   idx = np.argmax(tpr >= target_tpr)\n",
    "\n",
    "                   disease_name = 'Dementia' if disease == 'all_dementias' else disease.capitalize()\n",
    "\n",
    "                   if idx < len(thresholds) and tpr[idx] >= target_tpr:\n",
    "                       threshold_80 = thresholds[idx]\n",
    "                       fpr_80 = fpr[idx]\n",
    "                       tpr_80 = tpr[idx]\n",
    "\n",
    "                       # Calculate how many patients need to be screened\n",
    "                       prevalence = n_cases / n_patients\n",
    "\n",
    "                       # Number of patients needed to screen per case detected at 80% sensitivity\n",
    "                       patients_to_screen_per_case = 1 / (tpr_80 * prevalence) if (tpr_80 * prevalence) > 0 else float('inf')\n",
    "\n",
    "                       print(f\"\\n{disease_name} - {pred_year}-year prediction:\")\n",
    "                       print(f\"  Threshold for 80% detection: {threshold_80:.3f}\")\n",
    "                       print(f\"  Sensitivity (TPR): {tpr_80*100:.1f}%\")\n",
    "                       print(f\"  FPR: {fpr_80*100:.1f}%\")\n",
    "                       print(f\"  Prevalence in test set: {prevalence*100:.2f}% ({n_cases}/{n_patients})\")\n",
    "                       print(f\"  Number needed to screen: {patients_to_screen_per_case:.0f} patients per case detected\")\n",
    "                   else:\n",
    "                       print(f\"\\n{disease_name} - {pred_year}-year prediction:\")\n",
    "                       print(f\"  Cannot achieve 80% detection rate with available data\")\n",
    "                       print(f\"  Maximum achievable TPR: {max(tpr)*100:.1f}%\")\n",
    "\n",
    "           return df\n",
    "\n",
    "\n",
    "\n",
    "# Generate the performance table\n",
    "print(\"Generating comprehensive performance table...\")\n",
    "print(\"This will take several minutes due to bootstrap calculations...\")\n",
    "\n",
    "performance_table = generate_performance_table(\n",
    "    diseases=['all_dementias', 'alzheimer'],\n",
    "    prediction_years=[2, 5, 10],\n",
    "    age=65,\n",
    "    include_charlson_bmi=True,\n",
    "    exclude_mci_baseline=False,\n",
    "    include_deceased_as_zeros=True,\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE PERFORMANCE TABLE\")\n",
    "print(\"=\"*100)\n",
    "display(performance_table)\n",
    "\n",
    "# Save the table\n",
    "output_file = os.path.join(OUTPUT_DIR, 'performance_table.csv')\n",
    "performance_table.to_csv(output_file, index=False)\n",
    "print(f\"\\nTable saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
